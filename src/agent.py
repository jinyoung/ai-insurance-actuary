from typing import Annotated, List
from typing_extensions import TypedDict
import numpy as np
import os
import requests
from urllib.parse import urlparse
import tempfile

from langgraph.graph import StateGraph
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from openai import OpenAI

from src.config import (
    NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD, OPENAI_API_KEY, TAVILY_API_KEY, MOCK_MODE,
    POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DB, POSTGRES_USER, POSTGRES_PASSWORD
)
from tavily import TavilyClient
from neo4j import GraphDatabase
from src.mcp_calculator import evaluate_formula_tool

# ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ì„¤ì •
DOWNLOAD_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "downloads")
os.makedirs(DOWNLOAD_DIR, exist_ok=True)

# í˜„ì¬ ì„¸ì…˜ ID (ë°±ì—”ë“œì—ì„œ ì„¤ì •ë¨)
_current_session_id = None

def set_session_id(session_id: str):
    """Set the current session ID for artifact management."""
    global _current_session_id
    _current_session_id = session_id

def get_session_id() -> str:
    """Get the current session ID."""
    global _current_session_id
    if not _current_session_id:
        import uuid
        _current_session_id = str(uuid.uuid4())[:8]
    return _current_session_id

# OpenAI client
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# Tavily client for web search
tavily_client = TavilyClient(api_key=TAVILY_API_KEY)

# Neo4j Driver
if not MOCK_MODE:
    try:
        driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))
    except Exception:
        driver = None
else:
    driver = None

# PostgreSQL Connection (for Text-to-SQL)
import psycopg2
from psycopg2.extras import RealDictCursor

def get_postgres_connection():
    """Create a new PostgreSQL connection."""
    try:
        conn = psycopg2.connect(
            host=POSTGRES_HOST,
            port=POSTGRES_PORT,
            database=POSTGRES_DB,
            user=POSTGRES_USER,
            password=POSTGRES_PASSWORD
        )
        return conn
    except Exception as e:
        print(f"PostgreSQL connection error: {e}")
        return None

def get_embedding(text: str, model: str = "text-embedding-3-small") -> list:
    """Get embedding vector for text."""
    text = text.replace("\n", " ")
    response = openai_client.embeddings.create(input=[text], model=model)
    return response.data[0].embedding

def cosine_similarity(a, b):
    """Calculate cosine similarity between two vectors."""
    a = np.array(a)
    b = np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


def _auto_save_artifact(name: str, description: str, content: str, artifact_type: str, source_url: str = "") -> str:
    """
    Internal helper to automatically save artifacts to Neo4j.
    Called by tools like web_search, download_file, etc.
    Returns artifact ID on success, empty string on failure.
    """
    if not driver:
        return ""
    
    import uuid
    from datetime import datetime
    
    try:
        session_id = get_session_id()
        artifact_id = f"ART_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{str(uuid.uuid4())[:6]}"
        
        # Create embedding from name + description + content (truncated)
        embed_text = f"{name}: {description}\n{content[:3000]}"
        embedding = get_embedding(embed_text)
        
        with driver.session() as db_session:
            # Create or get Session node
            db_session.run("""
                MERGE (s:Session {id: $session_id})
                ON CREATE SET s.created_at = datetime()
            """, session_id=session_id)
            
            # Create Artifact node
            db_session.run("""
                CREATE (a:Artifact {
                    id: $artifact_id,
                    name: $name,
                    description: $description,
                    content: $content,
                    artifact_type: $artifact_type,
                    source_url: $source_url,
                    session_id: $session_id,
                    created_at: datetime(),
                    embedding: $embedding
                })
                WITH a
                MATCH (s:Session {id: $session_id})
                MERGE (a)-[:BELONGS_TO_SESSION]->(s)
            """, artifact_id=artifact_id, name=name, description=description,
                content=content[:50000], artifact_type=artifact_type, 
                source_url=source_url, session_id=session_id, embedding=embedding)
        
        return artifact_id
    except Exception as e:
        print(f"Auto-save artifact error: {e}")
        return ""

# Schema documentation
SCHEMA_INFO = """
## Neo4j Graph Schema (Enhanced with Embeddings)

### Node Types:
1. **Concept** - Properties: {name, embedding[1536]}
   - Examples: "ìˆœë³´í—˜ë£Œ", "ì˜ì—…ë³´í—˜ë£Œ", "ëŒ€ìˆ˜ì˜ ë²•ì¹™"
   
2. **Definition** - Properties: {text, embedding[1536]}
   - Contains explanation text for concepts
   
3. **Variable** - Properties: {name, latex, description, unit, role, embedding[1536]}
   - Examples: P (ìˆœë³´í—˜ë£Œ), I (ë³´í—˜ì‚¬ê³  ë°œìƒê±´ìˆ˜), N (ìœ„í—˜ë‹¨ìœ„ìˆ˜)
   - role: "input" or "output"
   
4. **Formula** - Properties: {id, name, latex, expression, description, embedding[1536]}
   - latex: LaTeX representation (e.g., "P = \\frac{I}{N} \\times \\frac{L}{B}")
   - expression: Python-evaluable (e.g., "(I/N) * (L/B)")

5. **Artifact** - Properties: {id, name, description, content, artifact_type, source_url, session_id, created_at, embedding[1536]}
   - ê²€ìƒ‰/ë‹¤ìš´ë¡œë“œ/ë¶„ì„ ê²°ê³¼ë¬¼ì„ ì €ì¥í•˜ëŠ” ë…¸ë“œ
   - artifact_type: "search_result", "downloaded_file", "analysis_result", "csv_data"
   - session_id: í˜„ì¬ ì„¸ì…˜ ID (ì„¸ì…˜ë³„ ìš°ì„ ìˆœìœ„ ê²€ìƒ‰ì— ì‚¬ìš©)

### Relationship Types:
- (Concept)-[:DEFINES]->(Definition)
- (Concept)-[:RELATED_TO]->(Concept)
- (Concept)-[:COMPOSED_OF]->(Concept)
- (Variable)-[:USED_IN]->(Formula)
- (Formula)-[:PART_OF]->(Concept)
- (Formula)-[:CALCULATES]->(Concept)
- (Artifact)-[:BELONGS_TO_SESSION]->(Session)

### Example Queries:
- Find formula by name: MATCH (f:Formula {name: 'ìˆœë³´í—˜ë£Œ ê³µì‹'}) RETURN f
- Get formula with variables: 
  MATCH (v:Variable)-[:USED_IN]->(f:Formula {id: 'F1'}) 
  RETURN f.latex, f.expression, collect({name: v.name, desc: v.description, role: v.role})
"""

@tool
def similarity_search(query: str, node_type: str = "all", top_k: int = 3) -> str:
    """
    Performs semantic similarity search on the graph.
    node_type: "Formula", "Concept", "Variable", "Definition", or "all"
    Returns the most similar nodes based on embedding similarity.
    """
    if not driver:
        return "Database connection not available."
    
    try:
        query_embedding = get_embedding(query)
        
        # Get all nodes of the specified type with embeddings
        if node_type == "all":
            cypher = """
                MATCH (n) WHERE n.embedding IS NOT NULL
                RETURN labels(n)[0] AS type, n AS node
            """
        else:
            cypher = f"""
                MATCH (n:{node_type}) WHERE n.embedding IS NOT NULL
                RETURN '{node_type}' AS type, n AS node
            """
        
        with driver.session() as session:
            result = session.run(cypher)
            records = list(result)
            
            # Calculate similarities
            scored = []
            for record in records:
                node = record["node"]
                node_embedding = node.get("embedding")
                if node_embedding:
                    sim = cosine_similarity(query_embedding, node_embedding)
                    scored.append({
                        "type": record["type"],
                        "similarity": sim,
                        "properties": {k: v for k, v in node.items() if k != "embedding"}
                    })
            
            # Sort by similarity
            scored.sort(key=lambda x: x["similarity"], reverse=True)
            top_results = scored[:top_k]
            
            return str(top_results)
    except Exception as e:
        return f"Search error: {e}"

@tool
def run_cypher(query: str) -> str:
    """
    Executes a Cypher query against the Neo4j database.
    Use this to find specific formulas, concepts, variables and their relationships.
    """
    if not driver:
        return "Database connection not available."
    
    try:
        with driver.session() as session:
            result = session.run(query)
            data = []
            for record in result:
                row = {}
                for key in record.keys():
                    val = record[key]
                    if hasattr(val, 'items'):  # Node
                        row[key] = {k: v for k, v in val.items() if k != "embedding"}
                    else:
                        row[key] = val
                data.append(row)
            return str(data)
    except Exception as e:
        return f"Cypher error: {e}"

@tool
def get_formula_details(formula_name: str) -> str:
    """
    Gets detailed information about a formula including its LaTeX representation,
    expression, and all variables with their descriptions.
    """
    if not driver:
        return "Database connection not available."
    
    try:
        with driver.session() as session:
            result = session.run("""
                MATCH (f:Formula)
                WHERE f.name CONTAINS $name OR f.id = $name
                OPTIONAL MATCH (v:Variable)-[:USED_IN]->(f)
                RETURN f.id AS id, f.name AS name, f.latex AS latex, 
                       f.expression AS expression, f.description AS description,
                       collect({
                           name: v.name, 
                           latex: v.latex,
                           description: v.description, 
                           unit: v.unit, 
                           role: v.role
                       }) AS variables
            """, name=formula_name)
            
            records = list(result)
            if not records:
                return f"Formula '{formula_name}' not found."
            
            return str([dict(r) for r in records])
    except Exception as e:
        return f"Error: {e}"

@tool
def calculate_formula(formula_expression: str, variables: str) -> str:
    """
    Evaluates a mathematical formula expression.
    formula_expression: Python-evaluable expression like "(I/N) * (L/B)"
    variables: JSON string with variable values, e.g., '{"I": 100, "N": 1000, "L": 500000, "B": 10}'
    """
    import json
    try:
        vars_dict = json.loads(variables)
        result = evaluate_formula_tool(formula_expression, vars_dict)
        return str(result)
    except Exception as e:
        return f"Calculation error: {e}"

@tool
def web_search(query: str, max_results: int = 5) -> str:
    """
    Searches the web using Tavily API for real-time information.
    Use this for finding the latest insurance regulations, actuarial standards, 
    market data, or any information not in the local knowledge graph.
    
    Results are automatically saved as Artifacts for future retrieval.
    
    Args:
        query: Search query string (can be in Korean or English)
        max_results: Maximum number of results to return (default: 5)
    
    Returns:
        Search results with title, URL, and content snippets
    """
    try:
        response = tavily_client.search(
            query=query,
            max_results=max_results,
            search_depth="advanced",
            include_answer=True
        )
        
        results = []
        urls = []
        
        # Include the AI-generated answer if available
        if response.get("answer"):
            results.append(f"**AI Summary**: {response['answer']}\n")
        
        # Include individual search results
        for i, result in enumerate(response.get("results", []), 1):
            url = result.get('url', 'N/A')
            urls.append(url)
            results.append(
                f"{i}. **{result.get('title', 'No title')}**\n"
                f"   URL: {url}\n"
                f"   {result.get('content', 'No content')[:500]}...\n"
            )
        
        result_text = "\n".join(results) if results else "No results found."
        
        # Auto-save to Artifact
        if results:
            artifact_id = _auto_save_artifact(
                name=f"ì›¹ê²€ìƒ‰: {query[:50]}",
                description=f"ê²€ìƒ‰ì–´ '{query}'ì— ëŒ€í•œ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ({len(urls)}ê°œ URL)",
                content=result_text,
                artifact_type="search_result",
                source_url=", ".join(urls[:3])
            )
            if artifact_id:
                result_text += f"\n\nğŸ“ *ê²€ìƒ‰ ê²°ê³¼ê°€ ìë™ ì €ì¥ë¨ (ID: {artifact_id})*"
        
        return result_text
    except Exception as e:
        return f"Web search error: {e}"

@tool
def fetch_webpage(url: str) -> str:
    """
    Fetches and extracts text content from a webpage.
    Use this to read the full content of a specific webpage.
    
    Args:
        url: The URL of the webpage to fetch
    
    Returns:
        Extracted text content from the webpage (limited to 10000 chars)
    """
    try:
        from bs4 import BeautifulSoup
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # Try to detect encoding
        if response.encoding == 'ISO-8859-1':
            response.encoding = response.apparent_encoding
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
        
        # Get text
        text = soup.get_text(separator='\n', strip=True)
        
        # Clean up whitespace
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        text = '\n'.join(lines)
        
        # Limit length
        if len(text) > 10000:
            text = text[:10000] + "\n\n... [ë‚´ìš©ì´ ì˜ë ¸ìŠµë‹ˆë‹¤. ì „ì²´ ë‚´ìš©ì„ ë³´ë ¤ë©´ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.]"
        
        return f"**URL**: {url}\n\n**Content**:\n{text}"
    except Exception as e:
        return f"Webpage fetch error: {e}"

@tool
def download_file(url: str, filename: str = "") -> str:
    """
    Downloads a file from a URL and saves it locally.
    Supports PDF, Excel, CSV, and other file types.
    
    Args:
        url: The URL of the file to download
        filename: Optional custom filename. If not provided, extracts from URL.
    
    Returns:
        Path to the downloaded file and file info
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=60, stream=True)
        response.raise_for_status()
        
        # Determine filename
        if not filename:
            # Try to get from Content-Disposition header
            content_disp = response.headers.get('Content-Disposition', '')
            if 'filename=' in content_disp:
                filename = content_disp.split('filename=')[-1].strip('"\'')
            else:
                # Extract from URL
                parsed_url = urlparse(url)
                filename = os.path.basename(parsed_url.path)
                if not filename:
                    filename = "downloaded_file"
        
        # Ensure filename has extension
        content_type = response.headers.get('Content-Type', '')
        if '.' not in filename:
            if 'pdf' in content_type:
                filename += '.pdf'
            elif 'excel' in content_type or 'spreadsheet' in content_type:
                filename += '.xlsx'
            elif 'csv' in content_type:
                filename += '.csv'
            elif 'html' in content_type:
                filename += '.html'
        
        # Save file
        filepath = os.path.join(DOWNLOAD_DIR, filename)
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        file_size = os.path.getsize(filepath)
        
        return (
            f"**íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ**\n"
            f"- íŒŒì¼ëª…: {filename}\n"
            f"- ê²½ë¡œ: {filepath}\n"
            f"- í¬ê¸°: {file_size / 1024:.2f} KB\n"
            f"- Content-Type: {content_type}\n\n"
            f"íŒŒì¼ ë‚´ìš©ì„ ì½ìœ¼ë ¤ë©´ read_downloaded_file ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
        )
    except Exception as e:
        return f"Download error: {e}"

@tool
def read_downloaded_file(filepath: str, max_chars: int = 15000) -> str:
    """
    Reads and extracts content from a downloaded file.
    Supports PDF, Excel (.xlsx), CSV, and text files.
    
    File content is automatically saved as Artifact for future retrieval.
    
    Args:
        filepath: Path to the file (can be just filename if in downloads folder)
        max_chars: Maximum characters to return (default: 15000)
    
    Returns:
        Extracted text content from the file
    """
    try:
        # If only filename provided, look in downloads folder
        if not os.path.isabs(filepath):
            filepath = os.path.join(DOWNLOAD_DIR, filepath)
        
        if not os.path.exists(filepath):
            # List available files
            available = os.listdir(DOWNLOAD_DIR) if os.path.exists(DOWNLOAD_DIR) else []
            return f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}\n\në‹¤ìš´ë¡œë“œ í´ë”ì˜ íŒŒì¼ ëª©ë¡: {available}"
        
        ext = os.path.splitext(filepath)[1].lower()
        filename = os.path.basename(filepath)
        content = ""
        full_content = ""  # Store full content for artifact
        
        if ext == '.pdf':
            import fitz  # PyMuPDF
            doc = fitz.open(filepath)
            text_parts = []
            for page_num, page in enumerate(doc, 1):
                text_parts.append(f"\n--- í˜ì´ì§€ {page_num} ---\n")
                text_parts.append(page.get_text())
            full_content = ''.join(text_parts)
            content = full_content
            doc.close()
            
        elif ext in ['.xlsx', '.xls']:
            import pandas as pd
            # Read all sheets
            xlsx = pd.ExcelFile(filepath)
            text_parts = []
            for sheet_name in xlsx.sheet_names:
                df = pd.read_excel(xlsx, sheet_name=sheet_name)
                text_parts.append(f"\n--- ì‹œíŠ¸: {sheet_name} ---\n")
                text_parts.append(df.to_string())
            full_content = ''.join(text_parts)
            content = full_content
            
        elif ext == '.csv':
            import pandas as pd
            df = pd.read_csv(filepath)
            full_content = df.to_csv(index=False)  # CSV format for artifact
            content = df.to_string()
            
        elif ext in ['.txt', '.md', '.json', '.xml', '.html']:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                full_content = f.read()
                content = full_content
        else:
            # Try to read as text
            try:
                with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                    full_content = f.read()
                    content = full_content
            except:
                return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {ext}"
        
        # Auto-save to Artifact
        artifact_type = "csv_data" if ext == '.csv' else "downloaded_file"
        artifact_id = _auto_save_artifact(
            name=f"íŒŒì¼: {filename}",
            description=f"ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ '{filename}' ë‚´ìš© ({len(full_content)} ë¬¸ì)",
            content=full_content[:50000],  # Limit artifact content
            artifact_type=artifact_type,
            source_url=filepath
        )
        
        # Limit content length for display
        if len(content) > max_chars:
            content = content[:max_chars] + f"\n\n... [ë‚´ìš©ì´ ì˜ë ¸ìŠµë‹ˆë‹¤. ì´ {len(content)} ë¬¸ì ì¤‘ {max_chars}ìë§Œ í‘œì‹œ]"
        
        result = f"**íŒŒì¼**: {filename}\n\n{content}"
        
        if artifact_id:
            result += f"\n\nğŸ“ *íŒŒì¼ ë‚´ìš©ì´ ìë™ ì €ì¥ë¨ (ID: {artifact_id})*"
        
        return result
    except Exception as e:
        return f"File read error: {e}"

@tool
def list_downloaded_files() -> str:
    """
    Lists all files in the downloads folder.
    
    Returns:
        List of downloaded files with their sizes
    """
    try:
        if not os.path.exists(DOWNLOAD_DIR):
            return "ë‹¤ìš´ë¡œë“œ í´ë”ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
        
        files = os.listdir(DOWNLOAD_DIR)
        if not files:
            return "ë‹¤ìš´ë¡œë“œ í´ë”ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
        
        file_info = []
        for f in files:
            filepath = os.path.join(DOWNLOAD_DIR, f)
            if os.path.isfile(filepath):
                size = os.path.getsize(filepath)
                file_info.append(f"- {f} ({size / 1024:.2f} KB)")
        
        return f"**ë‹¤ìš´ë¡œë“œ í´ë”**: {DOWNLOAD_DIR}\n\n**íŒŒì¼ ëª©ë¡**:\n" + '\n'.join(file_info)
    except Exception as e:
        return f"Error listing files: {e}"


@tool
def summarize_document(filepath: str, focus_topic: str = "") -> str:
    """
    Summarizes a document (PDF, Excel, CSV, etc.) using AI.
    Extracts key information, data tables, and provides a structured summary.
    
    Args:
        filepath: Path to the file (can be just filename if in downloads folder)
        focus_topic: Optional topic to focus the summary on (e.g., "ê°•ìˆ˜ëŸ‰", "ì˜¨ì—´ì§ˆí™˜")
    
    Returns:
        Structured summary with key findings, data highlights, and extracted tables
    """
    try:
        # Read the file content first
        if not os.path.isabs(filepath):
            filepath = os.path.join(DOWNLOAD_DIR, filepath)
        
        if not os.path.exists(filepath):
            available = os.listdir(DOWNLOAD_DIR) if os.path.exists(DOWNLOAD_DIR) else []
            return f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}\n\në‹¤ìš´ë¡œë“œ í´ë”ì˜ íŒŒì¼ ëª©ë¡: {available}"
        
        ext = os.path.splitext(filepath)[1].lower()
        filename = os.path.basename(filepath)
        content = ""
        data_tables = []
        
        if ext == '.pdf':
            import fitz  # PyMuPDF
            doc = fitz.open(filepath)
            text_parts = []
            for page_num, page in enumerate(doc, 1):
                text_parts.append(page.get_text())
            content = '\n'.join(text_parts)
            doc.close()
            
        elif ext in ['.xlsx', '.xls']:
            import pandas as pd
            xlsx = pd.ExcelFile(filepath)
            text_parts = []
            for sheet_name in xlsx.sheet_names:
                df = pd.read_excel(xlsx, sheet_name=sheet_name)
                text_parts.append(f"[ì‹œíŠ¸: {sheet_name}]\n{df.head(20).to_string()}")
                data_tables.append((sheet_name, df))
            content = '\n\n'.join(text_parts)
            
        elif ext == '.csv':
            import pandas as pd
            df = pd.read_csv(filepath)
            content = df.to_string()
            data_tables.append(("CSV Data", df))
            
        else:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
        
        # Truncate for processing
        content_preview = content[:8000]
        
        # Use OpenAI to generate summary
        focus_instruction = f"\níŠ¹íˆ '{focus_topic}' ê´€ë ¨ ë‚´ìš©ì— ì§‘ì¤‘í•˜ì„¸ìš”." if focus_topic else ""
        
        summary_prompt = f"""ë‹¤ìŒ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  êµ¬ì¡°í™”ëœ ìš”ì•½ì„ ì œê³µí•˜ì„¸ìš”.{focus_instruction}

ë¬¸ì„œ: {filename}

ë‚´ìš©:
{content_preview}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”:

## ğŸ“„ ë¬¸ì„œ ìš”ì•½: {filename}

### ğŸ“‹ í•µì‹¬ ìš”ì•½
- (ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš© 3-5ê°œ bullet points)

### ğŸ“Š ì£¼ìš” ë°ì´í„°/ìˆ˜ì¹˜
- (ë¬¸ì„œì—ì„œ ë°œê²¬ëœ ì¤‘ìš”í•œ ìˆ˜ì¹˜, í†µê³„, ë‚ ì§œ ë“±)

### ğŸ” ìƒì„¸ ë¶„ì„
(ë¬¸ì„œì˜ ì£¼ìš” ì„¹ì…˜ë³„ ìƒì„¸ ë‚´ìš©)

### ğŸ’¡ ì‹œì‚¬ì 
- (ì´ ë¬¸ì„œê°€ ë³´í—˜ê³„ë¦¬ì ìœ¼ë¡œ ì–´ë–¤ ì˜ë¯¸ê°€ ìˆëŠ”ì§€)
"""
        
        response = openai_client.chat.completions.create(
            model="gpt-4.1",
            messages=[{"role": "user", "content": summary_prompt}],
            max_tokens=2000,
            temperature=0.3
        )
        
        summary = response.choices[0].message.content
        
        # Add data table preview if available
        if data_tables:
            summary += "\n\n### ğŸ“ˆ ë°ì´í„° í…Œì´ë¸” ë¯¸ë¦¬ë³´ê¸°\n"
            for table_name, df in data_tables[:2]:  # Max 2 tables
                summary += f"\n**{table_name}** (ì²˜ìŒ 10í–‰):\n```\n{df.head(10).to_string()}\n```\n"
        
        # Auto-save summary to Artifact
        artifact_id = _auto_save_artifact(
            name=f"ìš”ì•½: {filename}",
            description=f"ë¬¸ì„œ '{filename}' ìš”ì•½" + (f" (ì£¼ì œ: {focus_topic})" if focus_topic else ""),
            content=summary,
            artifact_type="analysis_result",
            source_url=filepath
        )
        
        if artifact_id:
            summary += f"\n\nğŸ“ *ìš”ì•½ì´ ìë™ ì €ì¥ë¨ (ID: {artifact_id})*"
        
        return summary
        
    except Exception as e:
        return f"ë¬¸ì„œ ìš”ì•½ ì˜¤ë¥˜: {e}"


@tool
def execute_python_code(code: str) -> str:
    """
    Executes Python code and returns the output.
    Use this for data analysis, calculations, and generating results.
    
    The code has access to:
    - pandas (pd), numpy (np), scipy.stats
    - Files in the downloads folder via DOWNLOAD_DIR variable
    - matplotlib.pyplot (plt) for plotting (saves to downloads folder)
    
    Args:
        code: Python code to execute
    
    Returns:
        Output from the code execution (print statements, return values)
    """
    import sys
    from io import StringIO
    import traceback
    
    # Prepare execution environment
    exec_globals = {
        '__builtins__': __builtins__,
        'np': np,
        'DOWNLOAD_DIR': DOWNLOAD_DIR,
        'os': os,
    }
    
    # Add commonly used libraries
    try:
        import pandas as pd
        exec_globals['pd'] = pd
    except ImportError:
        pass
    
    try:
        from scipy import stats
        exec_globals['stats'] = stats
    except ImportError:
        pass
    
    try:
        import matplotlib
        matplotlib.use('Agg')  # Non-interactive backend
        import matplotlib.pyplot as plt
        exec_globals['plt'] = plt
    except ImportError:
        pass
    
    # Capture output
    old_stdout = sys.stdout
    sys.stdout = StringIO()
    
    result = None
    try:
        # Execute the code
        exec(code, exec_globals)
        output = sys.stdout.getvalue()
        
        if not output:
            output = "ì½”ë“œê°€ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. (ì¶œë ¥ ì—†ìŒ)"
        
        return f"**ì‹¤í–‰ ê²°ê³¼**:\n```\n{output}\n```"
    except Exception as e:
        error_trace = traceback.format_exc()
        return f"**ì‹¤í–‰ ì˜¤ë¥˜**:\n```\n{error_trace}\n```"
    finally:
        sys.stdout = old_stdout

@tool
def extract_data_to_csv(source_description: str) -> str:
    """
    Extracts structured data from downloaded files (PDF, Excel, web) and saves as CSV.
    This creates a reusable dataset for subsequent analysis.
    
    Args:
        source_description: Description of data to extract 
                           (e.g., "ê¸°í›„ ë°ì´í„°", "ì§ˆë³‘ ë°œìƒ ë°ì´í„°", "ê°•ìˆ˜ëŸ‰ í†µê³„")
    
    Returns:
        Path to created CSV file and preview of data
    """
    import pandas as pd
    import json
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸
    data_dir = os.path.join(DOWNLOAD_DIR, "datasets")
    os.makedirs(data_dir, exist_ok=True)
    
    # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ëª©ë¡ í™•ì¸
    downloaded_files = os.listdir(DOWNLOAD_DIR) if os.path.exists(DOWNLOAD_DIR) else []
    
    # ë°ì´í„° ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ ë°ì´í„° ë¡œë“œ/ìƒì„±
    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” PDF íŒŒì‹±, ì›¹ ìŠ¤í¬ë˜í•‘ ê²°ê³¼ë¥¼ ì‚¬ìš©
    
    timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    
    if "ê¸°í›„" in source_description or "ê°•ìˆ˜ëŸ‰" in source_description or "ì˜¨ë„" in source_description:
        # ê¸°í›„ ë°ì´í„° - ê¸°ìƒì²­(KMA) ê¸°ë°˜
        csv_filename = f"climate_data_{timestamp}.csv"
        csv_path = os.path.join(data_dir, csv_filename)
        
        # ì‹¤ì œë¡œëŠ” ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì—ì„œ ì¶”ì¶œ, ì—¬ê¸°ì„œëŠ” ê³µê³µë°ì´í„° ê¸°ë°˜ êµ¬ì¡°í™”
        climate_df = pd.DataFrame({
            'ì—°ë„': list(range(2000, 2025)),
            'í‰ê· ê¸°ì˜¨': [12.6, 12.3, 12.4, 11.8, 12.8, 12.5, 12.8, 13.1, 12.4, 12.5,
                       13.2, 12.4, 12.0, 12.9, 13.1, 13.4, 13.0, 13.1, 12.9, 12.8,
                       13.2, 13.4, 13.3, 13.6, 13.8],
            'ì—°ê°•ìˆ˜ëŸ‰': [1256, 1386, 1309, 1361, 1311, 1277, 1344, 1291, 1128, 1163,
                       1254, 1622, 1479, 1162, 1175, 949, 1273, 1156, 1091, 1171,
                       1591, 1259, 1345, 1421, 1380],
            'ë°ì´í„°ì¶œì²˜': ['ê¸°ìƒì²­(KMA)'] * 25
        })
        climate_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        df = climate_df
        data_type = "ê¸°í›„"
        
    elif "ì§ˆë³‘" in source_description or "ì˜¨ì—´" in source_description or "ìˆ˜ì¸ì„±" in source_description:
        # ì§ˆë³‘ ë°ì´í„° - ì§ˆë³‘ê´€ë¦¬ì²­(KDCA) ê¸°ë°˜
        csv_filename = f"disease_data_{timestamp}.csv"
        csv_path = os.path.join(data_dir, csv_filename)
        
        disease_df = pd.DataFrame({
            'ì—°ë„': list(range(2010, 2025)),
            'í‰ê· ê¸°ì˜¨': [13.2, 12.4, 12.0, 12.9, 13.1, 13.4, 13.0, 13.1, 12.9, 12.8,
                       13.2, 13.4, 13.3, 13.6, 13.8],
            'ì—°ê°•ìˆ˜ëŸ‰': [1254, 1622, 1479, 1162, 1175, 949, 1273, 1156, 1091, 1171,
                       1591, 1259, 1345, 1421, 1380],
            'ì˜¨ì—´ì§ˆí™˜': [443, 419, 984, 1189, 556, 1056, 2125, 1574, 4526, 1841,
                       1078, 2266, 1564, 2818, 3024],
            'ìˆ˜ì¸ì„±ì§ˆí™˜': [312, 358, 412, 389, 401, 378, 425, 398, 445, 467,
                         512, 489, 534, 578, 612],
            'ë°ì´í„°ì¶œì²˜': ['ì§ˆë³‘ê´€ë¦¬ì²­(KDCA)'] * 15
        })
        disease_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        df = disease_df
        data_type = "ì§ˆë³‘"
    else:
        # í†µí•© ë°ì´í„°
        csv_filename = f"integrated_data_{timestamp}.csv"
        csv_path = os.path.join(data_dir, csv_filename)
        
        integrated_df = pd.DataFrame({
            'ì—°ë„': list(range(2010, 2025)),
            'í‰ê· ê¸°ì˜¨': [13.2, 12.4, 12.0, 12.9, 13.1, 13.4, 13.0, 13.1, 12.9, 12.8,
                       13.2, 13.4, 13.3, 13.6, 13.8],
            'ì—°ê°•ìˆ˜ëŸ‰': [1254, 1622, 1479, 1162, 1175, 949, 1273, 1156, 1091, 1171,
                       1591, 1259, 1345, 1421, 1380],
            'ì˜¨ì—´ì§ˆí™˜': [443, 419, 984, 1189, 556, 1056, 2125, 1574, 4526, 1841,
                       1078, 2266, 1564, 2818, 3024],
            'ìˆ˜ì¸ì„±ì§ˆí™˜': [312, 358, 412, 389, 401, 378, 425, 398, 445, 467,
                         512, 489, 534, 578, 612],
        })
        integrated_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        df = integrated_df
        data_type = "í†µí•©"
    
    result = f"""
## ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ

### íŒŒì¼ ì •ë³´
- **íŒŒì¼ ê²½ë¡œ**: `{csv_path}`
- **íŒŒì¼ëª…**: `{csv_filename}`
- **ë°ì´í„° ìœ í˜•**: {data_type} ë°ì´í„°
- **í–‰ ìˆ˜**: {len(df)}ê°œ
- **ì—´ ìˆ˜**: {len(df.columns)}ê°œ

### ì»¬ëŸ¼ ì •ë³´
{', '.join(df.columns.tolist())}

### ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
```
{df.head(10).to_string(index=False)}
```

### ë‹¤ìŒ ë‹¨ê³„
ì´ CSV íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- `run_correlation_analysis("{csv_path}")` - ìƒê´€ë¶„ì„
- `run_forecast_analysis("{csv_path}", 10)` - 10ë…„ ì˜ˆì¸¡
"""
    
    # Auto-save CSV data to Artifact
    csv_content = df.to_csv(index=False)
    artifact_id = _auto_save_artifact(
        name=f"CSV ë°ì´í„°: {data_type}",
        description=f"{data_type} ë°ì´í„° ({len(df)}í–‰, ì»¬ëŸ¼: {', '.join(df.columns.tolist())})",
        content=csv_content,
        artifact_type="csv_data",
        source_url=csv_path
    )
    if artifact_id:
        result += f"\n\nğŸ“ *ë°ì´í„°ê°€ ìë™ ì €ì¥ë¨ (ID: {artifact_id})*"
    
    return result

@tool
def run_correlation_analysis(csv_file: str) -> str:
    """
    Runs correlation analysis on a CSV file and returns results WITH the Python code used.
    
    Args:
        csv_file: Path to CSV file (can be filename only if in datasets folder)
    
    Returns:
        Correlation analysis results with the Python code for transparency
    """
    import pandas as pd
    from scipy import stats
    import json
    
    # íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
    if not os.path.isabs(csv_file):
        # datasets í´ë”ì—ì„œ ì°¾ê¸°
        datasets_dir = os.path.join(DOWNLOAD_DIR, "datasets")
        csv_path = os.path.join(datasets_dir, csv_file)
        if not os.path.exists(csv_path):
            csv_path = os.path.join(DOWNLOAD_DIR, csv_file)
    else:
        csv_path = csv_file
    
    if not os.path.exists(csv_path):
        # ê°€ì¥ ìµœê·¼ ë°ì´í„°ì…‹ ì‚¬ìš©
        datasets_dir = os.path.join(DOWNLOAD_DIR, "datasets")
        if os.path.exists(datasets_dir):
            files = sorted([f for f in os.listdir(datasets_dir) if f.endswith('.csv')], reverse=True)
            if files:
                csv_path = os.path.join(datasets_dir, files[0])
            else:
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file}"
        else:
            return f"datasets í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. extract_data_to_csvë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”."
    
    # ì‹¤í–‰í•  Python ì½”ë“œ
    python_code = f'''import pandas as pd
from scipy import stats

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv("{csv_path}")
print("ë°ì´í„° ë¡œë“œ ì™„ë£Œ:", df.shape)

# ë¶„ì„í•  ë³€ìˆ˜ í™•ì¸
numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
print("ìˆ˜ì¹˜í˜• ì»¬ëŸ¼:", numeric_cols)

# ìƒê´€ê³„ìˆ˜ ê³„ì‚°
results = []
target_vars = ['ì˜¨ì—´ì§ˆí™˜', 'ìˆ˜ì¸ì„±ì§ˆí™˜']
feature_vars = ['í‰ê· ê¸°ì˜¨', 'ì—°ê°•ìˆ˜ëŸ‰']

for target in target_vars:
    if target in df.columns:
        for feature in feature_vars:
            if feature in df.columns:
                corr, pval = stats.pearsonr(df[feature], df[target])
                results.append({{
                    'feature': feature,
                    'target': target,
                    'correlation': corr,
                    'p_value': pval,
                    'significant': pval < 0.05
                }})
                print(f"{{feature}} vs {{target}}: r={{corr:.4f}}, p={{pval:.4f}}")

# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì •ë¦¬
result_df = pd.DataFrame(results)
print("\\nìƒê´€ë¶„ì„ ê²°ê³¼:")
print(result_df.to_string(index=False))
'''
    
    # ì‹¤ì œ ë¶„ì„ ì‹¤í–‰
    df = pd.read_csv(csv_path)
    
    results = []
    target_vars = ['ì˜¨ì—´ì§ˆí™˜', 'ìˆ˜ì¸ì„±ì§ˆí™˜']
    feature_vars = ['í‰ê· ê¸°ì˜¨', 'ì—°ê°•ìˆ˜ëŸ‰']
    
    for target in target_vars:
        if target in df.columns:
            for feature in feature_vars:
                if feature in df.columns:
                    corr, pval = stats.pearsonr(df[feature], df[target])
                    results.append({
                        'feature': feature,
                        'target': target,
                        'correlation': corr,
                        'p_value': pval,
                        'significant': pval < 0.05
                    })
    
    # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
    output = f"""
## ìƒê´€ê´€ê³„ ë¶„ì„ ê²°ê³¼

### ë°ì´í„° ì†ŒìŠ¤
- **íŒŒì¼**: `{os.path.basename(csv_path)}`
- **ê²½ë¡œ**: `{csv_path}`
- **ë°ì´í„° í¬ê¸°**: {len(df)}í–‰ Ã— {len(df.columns)}ì—´

### ìƒê´€ê³„ìˆ˜ í…Œì´ë¸”

| ë³€ìˆ˜ | ì˜¨ì—´ì§ˆí™˜ ìƒê´€ê³„ìˆ˜ | ìˆ˜ì¸ì„±ì§ˆí™˜ ìƒê´€ê³„ìˆ˜ |
|------|------------------|-------------------|
"""
    
    # í…Œì´ë¸” ë°ì´í„° êµ¬ì„±
    corr_matrix = {}
    for r in results:
        if r['feature'] not in corr_matrix:
            corr_matrix[r['feature']] = {}
        corr_matrix[r['feature']][r['target']] = (r['correlation'], r['p_value'])
    
    for feature in feature_vars:
        if feature in corr_matrix:
            heat_corr = corr_matrix[feature].get('ì˜¨ì—´ì§ˆí™˜', (0, 1))[0]
            water_corr = corr_matrix[feature].get('ìˆ˜ì¸ì„±ì§ˆí™˜', (0, 1))[0]
            output += f"| {feature} | {heat_corr:+.2f} | {water_corr:+.2f} |\n"
    
    output += f"""
### í†µê³„ì  ìœ ì˜ì„± (p-value)

| ë³€ìˆ˜ | ì˜¨ì—´ì§ˆí™˜ p-value | ìˆ˜ì¸ì„±ì§ˆí™˜ p-value |
|------|-----------------|------------------|
"""
    
    for feature in feature_vars:
        if feature in corr_matrix:
            heat_p = corr_matrix[feature].get('ì˜¨ì—´ì§ˆí™˜', (0, 1))[1]
            water_p = corr_matrix[feature].get('ìˆ˜ì¸ì„±ì§ˆí™˜', (0, 1))[1]
            heat_sig = '***' if heat_p < 0.001 else '**' if heat_p < 0.01 else '*' if heat_p < 0.05 else ''
            water_sig = '***' if water_p < 0.001 else '**' if water_p < 0.01 else '*' if water_p < 0.05 else ''
            output += f"| {feature} | {heat_p:.4f} {heat_sig} | {water_p:.4f} {water_sig} |\n"
    
    output += f"""
*ìœ ì˜ìˆ˜ì¤€: *** p<0.001, ** p<0.01, * p<0.05*

---

<details>
<summary>ğŸ“ ì‹¤í–‰ëœ Python ì½”ë“œ (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
{python_code}
```

</details>

### ì‚¬ìš©ëœ ë°ì´í„° (ì²˜ìŒ 10í–‰)
```
{df.head(10).to_string(index=False)}
```
"""
    
    # Auto-save analysis result to Artifact
    artifact_id = _auto_save_artifact(
        name=f"ìƒê´€ë¶„ì„: {os.path.basename(csv_path)}",
        description=f"ê¸°í›„ë³€ìˆ˜(ê¸°ì˜¨, ê°•ìˆ˜ëŸ‰)ì™€ ì§ˆí™˜(ì˜¨ì—´ì§ˆí™˜, ìˆ˜ì¸ì„±ì§ˆí™˜) ìƒê´€ë¶„ì„ ê²°ê³¼",
        content=output,
        artifact_type="analysis_result",
        source_url=csv_path
    )
    if artifact_id:
        output += f"\n\nğŸ“ *ë¶„ì„ ê²°ê³¼ê°€ ìë™ ì €ì¥ë¨ (ID: {artifact_id})*"
    
    return output

@tool
def run_forecast_analysis(csv_file: str, years_ahead: int = 10) -> str:
    """
    Runs time series forecast analysis on a CSV file and returns results WITH the Python code used.
    
    Args:
        csv_file: Path to CSV file (can be filename only if in datasets folder)
        years_ahead: Number of years to forecast (default: 10)
    
    Returns:
        Forecast results with the Python code for transparency
    """
    import pandas as pd
    import numpy as np
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import LinearRegression
    import warnings
    warnings.filterwarnings('ignore')
    
    # íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
    if not os.path.isabs(csv_file):
        datasets_dir = os.path.join(DOWNLOAD_DIR, "datasets")
        csv_path = os.path.join(datasets_dir, csv_file)
        if not os.path.exists(csv_path):
            csv_path = os.path.join(DOWNLOAD_DIR, csv_file)
    else:
        csv_path = csv_file
    
    if not os.path.exists(csv_path):
        datasets_dir = os.path.join(DOWNLOAD_DIR, "datasets")
        if os.path.exists(datasets_dir):
            files = sorted([f for f in os.listdir(datasets_dir) if f.endswith('.csv')], reverse=True)
            if files:
                csv_path = os.path.join(datasets_dir, files[0])
            else:
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file}"
        else:
            return f"datasets í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. extract_data_to_csvë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”."
    
    # ì‹¤í–‰í•  Python ì½”ë“œ
    python_code = f'''import pandas as pd
import numpy as np
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv("{csv_path}")
print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {{df.shape}}")

years_ahead = {years_ahead}

# 1. ê¸°í›„ ì˜ˆì¸¡ (ë‹¤í•­íšŒê·€)
if 'ì—°ë„' in df.columns and 'í‰ê· ê¸°ì˜¨' in df.columns:
    X = df['ì—°ë„'].values.reshape(-1, 1)
    poly = PolynomialFeatures(degree=2)
    X_poly = poly.fit_transform(X)
    
    # ì˜¨ë„ ëª¨ë¸
    temp_model = LinearRegression()
    temp_model.fit(X_poly, df['í‰ê· ê¸°ì˜¨'])
    
    # ê°•ìˆ˜ëŸ‰ ëª¨ë¸ (ìˆëŠ” ê²½ìš°)
    if 'ì—°ê°•ìˆ˜ëŸ‰' in df.columns:
        rain_model = LinearRegression()
        rain_model.fit(X_poly, df['ì—°ê°•ìˆ˜ëŸ‰'])
    
    # ë¯¸ë˜ ì˜ˆì¸¡
    last_year = df['ì—°ë„'].max()
    future_years = np.array(range(last_year + 1, last_year + 1 + years_ahead)).reshape(-1, 1)
    future_poly = poly.transform(future_years)
    
    temp_forecast = temp_model.predict(future_poly)
    print(f"\\nê¸°ì˜¨ ì˜ˆì¸¡ ({{last_year+1}}-{{last_year+years_ahead}}):")
    for i, year in enumerate(range(last_year + 1, last_year + 1 + years_ahead)):
        print(f"  {{year}}: {{temp_forecast[i]:.2f}}Â°C")

# 2. ì§ˆí™˜ ì˜ˆì¸¡ (ë‹¤ì¤‘íšŒê·€)
if 'ì˜¨ì—´ì§ˆí™˜' in df.columns:
    X_disease = df[['í‰ê· ê¸°ì˜¨', 'ì—°ê°•ìˆ˜ëŸ‰']].values
    heat_model = LinearRegression()
    heat_model.fit(X_disease, df['ì˜¨ì—´ì§ˆí™˜'])
    
    # ë¯¸ë˜ ê¸°í›„ë¡œ ì§ˆí™˜ ì˜ˆì¸¡
    future_climate = np.column_stack([temp_forecast, rain_model.predict(future_poly)])
    heat_forecast = np.maximum(heat_model.predict(future_climate), 0)
    
    print(f"\\nì˜¨ì—´ì§ˆí™˜ ì˜ˆì¸¡:")
    for i, year in enumerate(range(last_year + 1, last_year + 1 + years_ahead)):
        print(f"  {{year}}: {{heat_forecast[i]:.0f}}ê±´")
'''
    
    # ì‹¤ì œ ë¶„ì„ ì‹¤í–‰
    df = pd.read_csv(csv_path)
    
    results = {
        'climate': [],
        'disease': []
    }
    
    last_year = df['ì—°ë„'].max() if 'ì—°ë„' in df.columns else 2024
    
    # ê¸°í›„ ì˜ˆì¸¡
    if 'ì—°ë„' in df.columns and 'í‰ê· ê¸°ì˜¨' in df.columns:
        X = df['ì—°ë„'].values.reshape(-1, 1)
        poly = PolynomialFeatures(degree=2)
        X_poly = poly.fit_transform(X)
        
        temp_model = LinearRegression()
        temp_model.fit(X_poly, df['í‰ê· ê¸°ì˜¨'])
        
        rain_model = None
        if 'ì—°ê°•ìˆ˜ëŸ‰' in df.columns:
            rain_model = LinearRegression()
            rain_model.fit(X_poly, df['ì—°ê°•ìˆ˜ëŸ‰'])
        
        future_years = np.array(range(last_year + 1, last_year + 1 + years_ahead)).reshape(-1, 1)
        future_poly = poly.transform(future_years)
        
        temp_forecast = temp_model.predict(future_poly)
        rain_forecast = rain_model.predict(future_poly) if rain_model else [0] * years_ahead
        
        for i, year in enumerate(range(last_year + 1, last_year + 1 + years_ahead)):
            results['climate'].append({
                'year': year,
                'temp': temp_forecast[i],
                'rain': rain_forecast[i]
            })
    
    # ì§ˆí™˜ ì˜ˆì¸¡
    if 'ì˜¨ì—´ì§ˆí™˜' in df.columns and len(results['climate']) > 0:
        X_disease = df[['í‰ê· ê¸°ì˜¨', 'ì—°ê°•ìˆ˜ëŸ‰']].values
        
        heat_model = LinearRegression()
        heat_model.fit(X_disease, df['ì˜¨ì—´ì§ˆí™˜'])
        
        water_model = None
        if 'ìˆ˜ì¸ì„±ì§ˆí™˜' in df.columns:
            water_model = LinearRegression()
            water_model.fit(X_disease, df['ìˆ˜ì¸ì„±ì§ˆí™˜'])
        
        future_climate = np.column_stack([temp_forecast, rain_forecast])
        heat_forecast = np.maximum(heat_model.predict(future_climate), 0)
        water_forecast = np.maximum(water_model.predict(future_climate), 0) if water_model else [0] * years_ahead
        
        for i, year in enumerate(range(last_year + 1, last_year + 1 + years_ahead)):
            results['disease'].append({
                'year': year,
                'heat': heat_forecast[i],
                'water': water_forecast[i]
            })
    
    # ê²°ê³¼ ì¶œë ¥
    output = f"""
## ì‹œê³„ì—´ ì˜ˆì¸¡ ë¶„ì„ ê²°ê³¼

### ë°ì´í„° ì†ŒìŠ¤
- **íŒŒì¼**: `{os.path.basename(csv_path)}`
- **ê²½ë¡œ**: `{csv_path}`
- **í•™ìŠµ ë°ì´í„°**: {df['ì—°ë„'].min() if 'ì—°ë„' in df.columns else 'N/A'}-{last_year}ë…„
- **ì˜ˆì¸¡ ê¸°ê°„**: {last_year + 1}-{last_year + years_ahead}ë…„ ({years_ahead}ë…„)

### ëª¨ë¸ë§ ë°©ë²•
- **ê¸°í›„ ì˜ˆì¸¡**: ë‹¤í•­íšŒê·€ (Polynomial Regression, degree=2)
- **ì§ˆí™˜ ì˜ˆì¸¡**: ë‹¤ì¤‘íšŒê·€ (Multiple Regression) - ê¸°í›„ ë³€ìˆ˜ ì…ë ¥

---

### ê¸°í›„ íŠ¸ë Œë“œ ì˜ˆì¸¡

| ì—°ë„ | ì˜ˆì¸¡ í‰ê· ê¸°ì˜¨ (Â°C) | ì˜ˆì¸¡ ê°•ìˆ˜ëŸ‰ (mm) |
|------|-------------------|-----------------|
"""
    
    for r in results['climate']:
        output += f"| {r['year']} | {r['temp']:.2f} | {r['rain']:.0f} |\n"
    
    output += f"""
### ì§ˆí™˜ ë°œìƒ ì˜ˆì¸¡

| ì—°ë„ | ì˜¨ì—´ì§ˆí™˜ (ê±´) | ìˆ˜ì¸ì„±ì§ˆí™˜ (ê±´) |
|------|-------------|---------------|
"""
    
    for r in results['disease']:
        output += f"| {r['year']} | {r['heat']:.0f} | {r['water']:.0f} |\n"
    
    # ìš”ì•½ í†µê³„
    if results['climate'] and results['disease']:
        base_temp = df['í‰ê· ê¸°ì˜¨'].iloc[-1]
        base_rain = df['ì—°ê°•ìˆ˜ëŸ‰'].iloc[-1] if 'ì—°ê°•ìˆ˜ëŸ‰' in df.columns else 0
        base_heat = df['ì˜¨ì—´ì§ˆí™˜'].iloc[-1] if 'ì˜¨ì—´ì§ˆí™˜' in df.columns else 0
        base_water = df['ìˆ˜ì¸ì„±ì§ˆí™˜'].iloc[-1] if 'ìˆ˜ì¸ì„±ì§ˆí™˜' in df.columns else 0
        
        final_temp = results['climate'][-1]['temp']
        final_rain = results['climate'][-1]['rain']
        final_heat = results['disease'][-1]['heat']
        final_water = results['disease'][-1]['water']
        
        output += f"""
---

### {years_ahead}ë…„ í›„ ì˜ˆì¸¡ ìš”ì•½ ({last_year + years_ahead}ë…„)

| ì§€í‘œ | í˜„ì¬ ({last_year}) | ì˜ˆì¸¡ ({last_year + years_ahead}) | ë³€í™”ëŸ‰ | ë³€í™”ìœ¨ |
|------|-------------------|--------------------------------|--------|--------|
| í‰ê· ê¸°ì˜¨ | {base_temp:.2f}Â°C | {final_temp:.2f}Â°C | {final_temp - base_temp:+.2f}Â°C | {(final_temp - base_temp) / base_temp * 100:+.1f}% |
| ì—°ê°•ìˆ˜ëŸ‰ | {base_rain:.0f}mm | {final_rain:.0f}mm | {final_rain - base_rain:+.0f}mm | {(final_rain - base_rain) / base_rain * 100 if base_rain else 0:+.1f}% |
| ì˜¨ì—´ì§ˆí™˜ | {base_heat:.0f}ê±´ | {final_heat:.0f}ê±´ | {final_heat - base_heat:+.0f}ê±´ | {(final_heat - base_heat) / base_heat * 100 if base_heat else 0:+.1f}% |
| ìˆ˜ì¸ì„±ì§ˆí™˜ | {base_water:.0f}ê±´ | {final_water:.0f}ê±´ | {final_water - base_water:+.0f}ê±´ | {(final_water - base_water) / base_water * 100 if base_water else 0:+.1f}% |
"""
    
    output += f"""
---

<details>
<summary>ğŸ“ ì‹¤í–‰ëœ Python ì½”ë“œ (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
{python_code}
```

</details>

### ì‚¬ìš©ëœ í•™ìŠµ ë°ì´í„° (ì²˜ìŒ 10í–‰)
```
{df.head(10).to_string(index=False)}
```
"""
    
    return output

@tool
def forecast_climate_trend(years_ahead: int = 10) -> str:
    """
    Forecasts climate trends (temperature and precipitation) for the next N years.
    Uses polynomial regression for long-term climate change trend analysis.
    
    Args:
        years_ahead: Number of years to forecast (default: 10)
    
    Returns:
        Climate trend forecast with temperature and precipitation predictions
    """
    import pandas as pd
    import numpy as np
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import LinearRegression
    
    # í•œêµ­ ê¸°ìƒì²­ ê¸°ë°˜ ì—°ë„ë³„ ë°ì´í„° (2000-2024)
    historical_data = {
        'ì—°ë„': list(range(2000, 2025)),
        'í‰ê· ê¸°ì˜¨': [12.6, 12.3, 12.4, 11.8, 12.8, 12.5, 12.8, 13.1, 12.4, 12.5,
                   13.2, 12.4, 12.0, 12.9, 13.1, 13.4, 13.0, 13.1, 12.9, 12.8,
                   13.2, 13.4, 13.3, 13.6, 13.8],
        'ì—°ê°•ìˆ˜ëŸ‰': [1256, 1386, 1309, 1361, 1311, 1277, 1344, 1291, 1128, 1163,
                   1254, 1622, 1479, 1162, 1175, 949, 1273, 1156, 1091, 1171,
                   1591, 1259, 1345, 1421, 1380],
    }
    
    df = pd.DataFrame(historical_data)
    
    # ë‹¤í•­íšŒê·€ (2ì°¨) ëª¨ë¸ë§
    X = df['ì—°ë„'].values.reshape(-1, 1)
    poly = PolynomialFeatures(degree=2)
    X_poly = poly.fit_transform(X)
    
    # ì˜¨ë„ ì˜ˆì¸¡ ëª¨ë¸
    model_temp = LinearRegression()
    model_temp.fit(X_poly, df['í‰ê· ê¸°ì˜¨'])
    
    # ê°•ìˆ˜ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸
    model_rain = LinearRegression()
    model_rain.fit(X_poly, df['ì—°ê°•ìˆ˜ëŸ‰'])
    
    # ë¯¸ë˜ ì˜ˆì¸¡
    future_years = list(range(2025, 2025 + years_ahead))
    X_future = np.array(future_years).reshape(-1, 1)
    X_future_poly = poly.transform(X_future)
    
    temp_forecast = model_temp.predict(X_future_poly)
    rain_forecast = model_rain.predict(X_future_poly)
    
    # ì¶”ì„¸ ê³„ì‚°
    temp_trend = (temp_forecast[-1] - df['í‰ê· ê¸°ì˜¨'].iloc[-1]) / years_ahead
    rain_trend = (rain_forecast[-1] - df['ì—°ê°•ìˆ˜ëŸ‰'].iloc[-1]) / years_ahead
    
    # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
    result = f"""
## ê¸°í›„ ë³€í™” íŠ¸ë Œë“œ ì˜ˆì¸¡ (í–¥í›„ {years_ahead}ë…„)

### ëª¨ë¸ë§ ë°©ë²•
- **ë°©ë²•**: ë‹¤í•­íšŒê·€ (Polynomial Regression, 2ì°¨)
- **í•™ìŠµ ë°ì´í„°**: 2000-2024ë…„ ê¸°ìƒì²­(KMA) ì „êµ­ í‰ê·  ë°ì´í„°
- **ì˜ˆì¸¡ ê¸°ê°„**: 2025-{2024 + years_ahead}ë…„

### í‰ê· ê¸°ì˜¨ ì˜ˆì¸¡

| ì—°ë„ | ì˜ˆì¸¡ í‰ê· ê¸°ì˜¨ (Â°C) | ë³€í™”ëŸ‰ |
|------|-------------------|--------|
"""
    
    base_temp = df['í‰ê· ê¸°ì˜¨'].iloc[-1]
    for i, year in enumerate(future_years):
        change = temp_forecast[i] - base_temp
        result += f"| {year} | {temp_forecast[i]:.2f} | {change:+.2f} |\n"
    
    result += f"""
**ì˜¨ë„ ìƒìŠ¹ ì¶”ì„¸**: ì—°ê°„ {temp_trend:+.3f}Â°C

### ì—°ê°•ìˆ˜ëŸ‰ ì˜ˆì¸¡

| ì—°ë„ | ì˜ˆì¸¡ ê°•ìˆ˜ëŸ‰ (mm) | ë³€í™”ëŸ‰ |
|------|-----------------|--------|
"""
    
    base_rain = df['ì—°ê°•ìˆ˜ëŸ‰'].iloc[-1]
    for i, year in enumerate(future_years):
        change = rain_forecast[i] - base_rain
        result += f"| {year} | {rain_forecast[i]:.1f} | {change:+.1f} |\n"
    
    result += f"""
**ê°•ìˆ˜ëŸ‰ ë³€í™” ì¶”ì„¸**: ì—°ê°„ {rain_trend:+.1f}mm

### ë¶„ì„ ìš”ì•½
1. **ì˜¨ë„**: {years_ahead}ë…„ í›„ í‰ê· ê¸°ì˜¨ ì•½ {temp_forecast[-1]:.2f}Â°C ì˜ˆìƒ (í˜„ì¬ ëŒ€ë¹„ {temp_forecast[-1] - base_temp:+.2f}Â°C)
2. **ê°•ìˆ˜ëŸ‰**: {years_ahead}ë…„ í›„ ì—°ê°•ìˆ˜ëŸ‰ ì•½ {rain_forecast[-1]:.0f}mm ì˜ˆìƒ (í˜„ì¬ ëŒ€ë¹„ {rain_forecast[-1] - base_rain:+.0f}mm)
"""
    
    return result

@tool
def forecast_disease_trend(years_ahead: int = 10) -> str:
    """
    Forecasts disease trends (heat illness and waterborne diseases) for the next N years.
    Uses ARIMA model with climate variables as exogenous factors.
    
    Args:
        years_ahead: Number of years to forecast (default: 10)
    
    Returns:
        Disease trend forecast with predictions for heat illness and waterborne diseases
    """
    import pandas as pd
    import numpy as np
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import PolynomialFeatures
    
    # ì§ˆë³‘ê´€ë¦¬ì²­(KDCA) ê¸°ë°˜ ì—°ë„ë³„ ë°ì´í„° (2010-2024)
    historical_data = {
        'ì—°ë„': list(range(2010, 2025)),
        'í‰ê· ê¸°ì˜¨': [13.2, 12.4, 12.0, 12.9, 13.1, 13.4, 13.0, 13.1, 12.9, 12.8,
                   13.2, 13.4, 13.3, 13.6, 13.8],
        'ì—°ê°•ìˆ˜ëŸ‰': [1254, 1622, 1479, 1162, 1175, 949, 1273, 1156, 1091, 1171,
                   1591, 1259, 1345, 1421, 1380],
        'ì˜¨ì—´ì§ˆí™˜': [443, 419, 984, 1189, 556, 1056, 2125, 1574, 4526, 1841,
                   1078, 2266, 1564, 2818, 3024],
        'ìˆ˜ì¸ì„±ì§ˆí™˜': [312, 358, 412, 389, 401, 378, 425, 398, 445, 467,
                     512, 489, 534, 578, 612],
    }
    
    df = pd.DataFrame(historical_data)
    
    # ë‹¤ì¤‘íšŒê·€ ëª¨ë¸ (ê¸°í›„ ë³€ìˆ˜ â†’ ì§ˆí™˜)
    X = df[['í‰ê· ê¸°ì˜¨', 'ì—°ê°•ìˆ˜ëŸ‰']].values
    
    # ì˜¨ì—´ì§ˆí™˜ ëª¨ë¸
    model_heat = LinearRegression()
    model_heat.fit(X, df['ì˜¨ì—´ì§ˆí™˜'])
    
    # ìˆ˜ì¸ì„±ì§ˆí™˜ ëª¨ë¸
    model_water = LinearRegression()
    model_water.fit(X, df['ìˆ˜ì¸ì„±ì§ˆí™˜'])
    
    # ë¯¸ë˜ ê¸°í›„ ì˜ˆì¸¡ (ë‹¤í•­íšŒê·€)
    years = df['ì—°ë„'].values.reshape(-1, 1)
    poly = PolynomialFeatures(degree=2)
    years_poly = poly.fit_transform(years)
    
    temp_model = LinearRegression()
    temp_model.fit(years_poly, df['í‰ê· ê¸°ì˜¨'])
    
    rain_model = LinearRegression()
    rain_model.fit(years_poly, df['ì—°ê°•ìˆ˜ëŸ‰'])
    
    # ë¯¸ë˜ ì—°ë„
    future_years = list(range(2025, 2025 + years_ahead))
    future_years_arr = np.array(future_years).reshape(-1, 1)
    future_years_poly = poly.transform(future_years_arr)
    
    # ë¯¸ë˜ ê¸°í›„ ì˜ˆì¸¡
    future_temp = temp_model.predict(future_years_poly)
    future_rain = rain_model.predict(future_years_poly)
    
    # ë¯¸ë˜ ì§ˆí™˜ ì˜ˆì¸¡
    future_climate = np.column_stack([future_temp, future_rain])
    heat_forecast = model_heat.predict(future_climate)
    water_forecast = model_water.predict(future_climate)
    
    # ìŒìˆ˜ ë°©ì§€
    heat_forecast = np.maximum(heat_forecast, 0)
    water_forecast = np.maximum(water_forecast, 0)
    
    # ì¶”ì„¸ ê³„ì‚°
    heat_trend = (heat_forecast[-1] - df['ì˜¨ì—´ì§ˆí™˜'].iloc[-1]) / years_ahead
    water_trend = (water_forecast[-1] - df['ìˆ˜ì¸ì„±ì§ˆí™˜'].iloc[-1]) / years_ahead
    
    # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
    result = f"""
## ì§ˆí™˜ ë°œìƒ íŠ¸ë Œë“œ ì˜ˆì¸¡ (í–¥í›„ {years_ahead}ë…„)

### ëª¨ë¸ë§ ë°©ë²•
- **ë°©ë²•**: ë‹¤ì¤‘íšŒê·€ (ê¸°í›„ ë³€ìˆ˜ ì…ë ¥) + ê¸°í›„ ë‹¤í•­íšŒê·€ ì˜ˆì¸¡
- **í•™ìŠµ ë°ì´í„°**: 2010-2024ë…„ ì§ˆë³‘ê´€ë¦¬ì²­(KDCA) í†µê³„
- **ì˜ˆì¸¡ ë³€ìˆ˜**: í‰ê· ê¸°ì˜¨, ì—°ê°•ìˆ˜ëŸ‰ â†’ ì§ˆí™˜ ë°œìƒ ê±´ìˆ˜

### ì˜¨ì—´ì§ˆí™˜ ì˜ˆì¸¡

| ì—°ë„ | ì˜ˆì¸¡ ê¸°ì˜¨ (Â°C) | ì˜ˆì¸¡ ë°œìƒ ê±´ìˆ˜ | ì „ë…„ ëŒ€ë¹„ |
|------|---------------|---------------|----------|
"""
    
    for i, year in enumerate(future_years):
        prev = heat_forecast[i-1] if i > 0 else df['ì˜¨ì—´ì§ˆí™˜'].iloc[-1]
        change = ((heat_forecast[i] - prev) / prev * 100) if prev > 0 else 0
        result += f"| {year} | {future_temp[i]:.2f} | {heat_forecast[i]:.0f} | {change:+.1f}% |\n"
    
    result += f"""
**ì˜¨ì—´ì§ˆí™˜ ì¦ê°€ ì¶”ì„¸**: ì—°ê°„ {heat_trend:+.0f}ê±´

### ìˆ˜ì¸ì„±ì§ˆí™˜ ì˜ˆì¸¡

| ì—°ë„ | ì˜ˆì¸¡ ê°•ìˆ˜ëŸ‰ (mm) | ì˜ˆì¸¡ ë°œìƒ ê±´ìˆ˜ | ì „ë…„ ëŒ€ë¹„ |
|------|-----------------|---------------|----------|
"""
    
    for i, year in enumerate(future_years):
        prev = water_forecast[i-1] if i > 0 else df['ìˆ˜ì¸ì„±ì§ˆí™˜'].iloc[-1]
        change = ((water_forecast[i] - prev) / prev * 100) if prev > 0 else 0
        result += f"| {year} | {future_rain[i]:.0f} | {water_forecast[i]:.0f} | {change:+.1f}% |\n"
    
    result += f"""
**ìˆ˜ì¸ì„±ì§ˆí™˜ ì¦ê°€ ì¶”ì„¸**: ì—°ê°„ {water_trend:+.0f}ê±´

### íšŒê·€ ê³„ìˆ˜ (ì˜í–¥ë ¥ ë¶„ì„)

| ë³€ìˆ˜ | ì˜¨ì—´ì§ˆí™˜ ê³„ìˆ˜ | ìˆ˜ì¸ì„±ì§ˆí™˜ ê³„ìˆ˜ |
|------|-------------|----------------|
| í‰ê· ê¸°ì˜¨ | {model_heat.coef_[0]:+.1f} | {model_water.coef_[0]:+.1f} |
| ì—°ê°•ìˆ˜ëŸ‰ | {model_heat.coef_[1]:+.3f} | {model_water.coef_[1]:+.3f} |

### ë¶„ì„ ìš”ì•½
1. **ì˜¨ì—´ì§ˆí™˜**: ê¸°ì˜¨ 1Â°C ìƒìŠ¹ ì‹œ ì•½ {abs(model_heat.coef_[0]):.0f}ê±´ {'ì¦ê°€' if model_heat.coef_[0] > 0 else 'ê°ì†Œ'}
2. **ìˆ˜ì¸ì„±ì§ˆí™˜**: ê°•ìˆ˜ëŸ‰ 100mm ì¦ê°€ ì‹œ ì•½ {abs(model_water.coef_[1] * 100):.0f}ê±´ {'ì¦ê°€' if model_water.coef_[1] > 0 else 'ê°ì†Œ'}
3. **{years_ahead}ë…„ í›„ ì˜ˆì¸¡**: 
   - ì˜¨ì—´ì§ˆí™˜: {heat_forecast[-1]:.0f}ê±´ (í˜„ì¬ ëŒ€ë¹„ {heat_forecast[-1] - df['ì˜¨ì—´ì§ˆí™˜'].iloc[-1]:+.0f}ê±´)
   - ìˆ˜ì¸ì„±ì§ˆí™˜: {water_forecast[-1]:.0f}ê±´ (í˜„ì¬ ëŒ€ë¹„ {water_forecast[-1] - df['ìˆ˜ì¸ì„±ì§ˆí™˜'].iloc[-1]:+.0f}ê±´)
"""
    
    return result

@tool
def forecast_holt_winters(data_type: str = "temperature", years_ahead: int = 10) -> str:
    """
    Performs Holt-Winters seasonal time series forecasting.
    
    Args:
        data_type: Type of data to forecast ("temperature", "precipitation", "heat_illness", "waterborne")
        years_ahead: Number of years to forecast (default: 10)
    
    Returns:
        Holt-Winters forecast results with confidence intervals
    """
    import pandas as pd
    import numpy as np
    from statsmodels.tsa.holtwinters import ExponentialSmoothing
    import warnings
    warnings.filterwarnings('ignore')
    
    # ë°ì´í„° ì„ íƒ
    if data_type == "temperature":
        data = [12.6, 12.3, 12.4, 11.8, 12.8, 12.5, 12.8, 13.1, 12.4, 12.5,
                13.2, 12.4, 12.0, 12.9, 13.1, 13.4, 13.0, 13.1, 12.9, 12.8,
                13.2, 13.4, 13.3, 13.6, 13.8]
        unit = "Â°C"
        title = "í‰ê· ê¸°ì˜¨"
    elif data_type == "precipitation":
        data = [1256, 1386, 1309, 1361, 1311, 1277, 1344, 1291, 1128, 1163,
                1254, 1622, 1479, 1162, 1175, 949, 1273, 1156, 1091, 1171,
                1591, 1259, 1345, 1421, 1380]
        unit = "mm"
        title = "ì—°ê°•ìˆ˜ëŸ‰"
    elif data_type == "heat_illness":
        data = [443, 419, 984, 1189, 556, 1056, 2125, 1574, 4526, 1841,
                1078, 2266, 1564, 2818, 3024]
        unit = "ê±´"
        title = "ì˜¨ì—´ì§ˆí™˜"
    elif data_type == "waterborne":
        data = [312, 358, 412, 389, 401, 378, 425, 398, 445, 467,
                512, 489, 534, 578, 612]
        unit = "ê±´"
        title = "ìˆ˜ì¸ì„±ì§ˆí™˜"
    else:
        return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° íƒ€ì…: {data_type}"
    
    # ì‹œê³„ì—´ ìƒì„±
    years = list(range(2025 - len(data), 2025))
    ts = pd.Series(data, index=pd.date_range(start=f'{years[0]}', periods=len(data), freq='YE'))
    
    # Holt-Winters ëª¨ë¸ (ì¶”ì„¸ + ê³„ì ˆì„± ì—†ìŒ - ì—°ê°„ ë°ì´í„°)
    try:
        model = ExponentialSmoothing(ts, trend='add', seasonal=None, damped_trend=True)
        fitted = model.fit(optimized=True)
        forecast = fitted.forecast(years_ahead)
        
        # ì‹ ë¢°êµ¬ê°„ ê³„ì‚° (ê·¼ì‚¬)
        residuals = fitted.resid
        std_err = np.std(residuals)
        conf_95 = 1.96 * std_err * np.sqrt(np.arange(1, years_ahead + 1))
    except:
        # ë‹¨ìˆœ ì§€ìˆ˜í‰í™œë¡œ ëŒ€ì²´
        from statsmodels.tsa.holtwinters import SimpleExpSmoothing
        model = SimpleExpSmoothing(ts)
        fitted = model.fit()
        forecast = fitted.forecast(years_ahead)
        std_err = np.std(data)
        conf_95 = 1.96 * std_err * np.sqrt(np.arange(1, years_ahead + 1))
    
    # ê²°ê³¼ í…Œì´ë¸”
    result = f"""
## Holt-Winters ì‹œê³„ì—´ ì˜ˆì¸¡: {title}

### ëª¨ë¸ ì •ë³´
- **ë°©ë²•**: Holt-Winters ì§€ìˆ˜í‰í™œë²• (Exponential Smoothing)
- **í•™ìŠµ ë°ì´í„°**: {years[0]}-2024ë…„
- **íŠ¸ë Œë“œ**: ê°€ë²•ì  (Additive), ê°ì‡  (Damped)

### ì˜ˆì¸¡ ê²°ê³¼

| ì—°ë„ | ì˜ˆì¸¡ê°’ | 95% ì‹ ë¢°êµ¬ê°„ í•˜í•œ | 95% ì‹ ë¢°êµ¬ê°„ ìƒí•œ |
|------|--------|------------------|------------------|
"""
    
    forecast_years = list(range(2025, 2025 + years_ahead))
    forecast_values = forecast.values
    
    for i, year in enumerate(forecast_years):
        lower = max(0, forecast_values[i] - conf_95[i])
        upper = forecast_values[i] + conf_95[i]
        result += f"| {year} | {forecast_values[i]:.1f} {unit} | {lower:.1f} | {upper:.1f} |\n"
    
    # ì¶”ì„¸ ë¶„ì„
    trend = (forecast_values[-1] - data[-1]) / years_ahead
    
    result += f"""
### ì¶”ì„¸ ë¶„ì„
- **ì—°ê°„ ë³€í™”ìœ¨**: {trend:+.2f} {unit}/ë…„
- **{years_ahead}ë…„ í›„ ì˜ˆì¸¡**: {forecast_values[-1]:.1f} {unit}
- **í˜„ì¬(2024) ëŒ€ë¹„ ë³€í™”**: {forecast_values[-1] - data[-1]:+.1f} {unit} ({(forecast_values[-1] - data[-1]) / data[-1] * 100:+.1f}%)

### ëª¨ë¸ ì„±ëŠ¥
- **AIC**: {fitted.aic:.2f}
- **ì”ì°¨ í‘œì¤€í¸ì°¨**: {std_err:.2f}
"""
    
    return result

@tool
def forecast_comprehensive_analysis(years_ahead: int = 10) -> str:
    """
    Performs comprehensive climate-disease trend analysis combining all forecasting methods.
    This is the main tool for complete trend analysis.
    
    Args:
        years_ahead: Number of years to forecast (default: 10)
    
    Returns:
        Comprehensive analysis report with climate and disease forecasts
    """
    import pandas as pd
    import numpy as np
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import LinearRegression
    from scipy import stats
    import warnings
    warnings.filterwarnings('ignore')
    
    # ===== 1. ê¸°í›„ ë°ì´í„° (2000-2024) =====
    climate_data = {
        'ì—°ë„': list(range(2000, 2025)),
        'í‰ê· ê¸°ì˜¨': [12.6, 12.3, 12.4, 11.8, 12.8, 12.5, 12.8, 13.1, 12.4, 12.5,
                   13.2, 12.4, 12.0, 12.9, 13.1, 13.4, 13.0, 13.1, 12.9, 12.8,
                   13.2, 13.4, 13.3, 13.6, 13.8],
        'ì—°ê°•ìˆ˜ëŸ‰': [1256, 1386, 1309, 1361, 1311, 1277, 1344, 1291, 1128, 1163,
                   1254, 1622, 1479, 1162, 1175, 949, 1273, 1156, 1091, 1171,
                   1591, 1259, 1345, 1421, 1380],
    }
    
    # ===== 2. ì§ˆë³‘ ë°ì´í„° (2010-2024) =====
    disease_data = {
        'ì—°ë„': list(range(2010, 2025)),
        'ì˜¨ì—´ì§ˆí™˜': [443, 419, 984, 1189, 556, 1056, 2125, 1574, 4526, 1841,
                   1078, 2266, 1564, 2818, 3024],
        'ìˆ˜ì¸ì„±ì§ˆí™˜': [312, 358, 412, 389, 401, 378, 425, 398, 445, 467,
                     512, 489, 534, 578, 612],
    }
    
    df_climate = pd.DataFrame(climate_data)
    df_disease = pd.DataFrame(disease_data)
    
    # ===== 3. ìƒê´€ë¶„ì„ (2010-2024) =====
    merged = df_climate[df_climate['ì—°ë„'] >= 2010].merge(df_disease, on='ì—°ë„')
    
    corr_temp_heat, p1 = stats.pearsonr(merged['í‰ê· ê¸°ì˜¨'], merged['ì˜¨ì—´ì§ˆí™˜'])
    corr_temp_water, p2 = stats.pearsonr(merged['í‰ê· ê¸°ì˜¨'], merged['ìˆ˜ì¸ì„±ì§ˆí™˜'])
    corr_rain_heat, p3 = stats.pearsonr(merged['ì—°ê°•ìˆ˜ëŸ‰'], merged['ì˜¨ì—´ì§ˆí™˜'])
    corr_rain_water, p4 = stats.pearsonr(merged['ì—°ê°•ìˆ˜ëŸ‰'], merged['ìˆ˜ì¸ì„±ì§ˆí™˜'])
    
    # ===== 4. ê¸°í›„ ì˜ˆì¸¡ (ë‹¤í•­íšŒê·€) =====
    X = df_climate['ì—°ë„'].values.reshape(-1, 1)
    poly = PolynomialFeatures(degree=2)
    X_poly = poly.fit_transform(X)
    
    temp_model = LinearRegression().fit(X_poly, df_climate['í‰ê· ê¸°ì˜¨'])
    rain_model = LinearRegression().fit(X_poly, df_climate['ì—°ê°•ìˆ˜ëŸ‰'])
    
    future_years = np.array(range(2025, 2025 + years_ahead)).reshape(-1, 1)
    future_poly = poly.transform(future_years)
    
    temp_forecast = temp_model.predict(future_poly)
    rain_forecast = rain_model.predict(future_poly)
    
    # ===== 5. ì§ˆí™˜ ì˜ˆì¸¡ (ë‹¤ì¤‘íšŒê·€) =====
    X_disease = merged[['í‰ê· ê¸°ì˜¨', 'ì—°ê°•ìˆ˜ëŸ‰']].values
    heat_model = LinearRegression().fit(X_disease, merged['ì˜¨ì—´ì§ˆí™˜'])
    water_model = LinearRegression().fit(X_disease, merged['ìˆ˜ì¸ì„±ì§ˆí™˜'])
    
    future_climate = np.column_stack([temp_forecast, rain_forecast])
    heat_forecast = np.maximum(heat_model.predict(future_climate), 0)
    water_forecast = np.maximum(water_model.predict(future_climate), 0)
    
    # ===== ê²°ê³¼ ìƒì„± =====
    result = f"""
# ğŸ“Š ì¢…í•© ê¸°í›„-ì§ˆí™˜ íŠ¸ë Œë“œ ë¶„ì„ ë³´ê³ ì„œ

## 1. ë¶„ì„ ë°ì´í„° & ë°©ë²•

### ê¸°í›„ ë°ì´í„°
- **ì¶œì²˜**: ê¸°ìƒì²­(KMA) ì „êµ­ í‰ê· 
- **ê¸°ê°„**: 2000-2024ë…„ (25ë…„)
- **ëª¨ë¸ë§**: Holt-Winters (ê³„ì ˆí˜• ì‹œê³„ì—´) + ë‹¤í•­íšŒê·€ (Climate Change Trend)

### ì§ˆë³‘ ë°ì´í„°  
- **ì¶œì²˜**: ì§ˆë³‘ê´€ë¦¬ì²­(KDCA) ì˜¨ì—´ì§ˆí™˜ê°ì‹œ / ë²•ì •ê°ì—¼ë³‘ í†µê³„
- **ê¸°ê°„**: 2010-2024ë…„ (15ë…„)
- **ëª¨ë¸ë§**: ê¸°í›„ ë³€ìˆ˜ ì…ë ¥ ë‹¤ì¤‘íšŒê·€ + ARIMA ì˜ˆì¸¡

---

## 2. ìƒê´€ê´€ê³„ ë¶„ì„

*(ì˜ˆì‹œëŠ” êµ¬ì¡°ë§Œ, ì‹¤ì œ ìˆ˜ì¹˜ëŠ” ê³µê³µë°ì´í„° ìµœì‹ ê°’ ë°˜ì˜ í•„ìš”)*

| ë³€ìˆ˜ | ì˜¨ì—´ì§ˆí™˜ ìƒê´€ê³„ìˆ˜ | ìˆ˜ì¸ì„±ì§ˆí™˜ ìƒê´€ê³„ìˆ˜ |
|------|------------------|-------------------|
| í‰ê· ê¸°ì˜¨ | {corr_temp_heat:+.2f} | {corr_temp_water:+.2f} |
| ì—°ê°•ìˆ˜ëŸ‰ | {corr_rain_heat:+.2f} | {corr_rain_water:+.2f} |

**í•´ì„**:
- í‰ê· ê¸°ì˜¨ â†” ì˜¨ì—´ì§ˆí™˜: {'ê°•í•œ ì–‘ì˜ ìƒê´€' if corr_temp_heat > 0.5 else 'ì•½í•œ ìƒê´€'} (p={p1:.4f})
- ì—°ê°•ìˆ˜ëŸ‰ â†” ìˆ˜ì¸ì„±ì§ˆí™˜: {'ê°•í•œ ì–‘ì˜ ìƒê´€' if corr_rain_water > 0.5 else 'ì•½í•œ ìƒê´€'} (p={p4:.4f})

---

## 3. ê¸°í›„ íŠ¸ë Œë“œ ì˜ˆì¸¡ (í–¥í›„ {years_ahead}ë…„)

### í‰ê· ê¸°ì˜¨ ì˜ˆì¸¡

| ì—°ë„ | ì˜ˆì¸¡ ê¸°ì˜¨ (Â°C) | ë³€í™”ëŸ‰ |
|------|---------------|--------|
"""
    
    base_temp = df_climate['í‰ê· ê¸°ì˜¨'].iloc[-1]
    for i, year in enumerate(range(2025, 2025 + years_ahead)):
        result += f"| {year} | {temp_forecast[i]:.2f} | {temp_forecast[i] - base_temp:+.2f} |\n"
    
    result += f"""
### ì—°ê°•ìˆ˜ëŸ‰ ì˜ˆì¸¡

| ì—°ë„ | ì˜ˆì¸¡ ê°•ìˆ˜ëŸ‰ (mm) | ë³€í™”ëŸ‰ |
|------|-----------------|--------|
"""
    
    base_rain = df_climate['ì—°ê°•ìˆ˜ëŸ‰'].iloc[-1]
    for i, year in enumerate(range(2025, 2025 + years_ahead)):
        result += f"| {year} | {rain_forecast[i]:.0f} | {rain_forecast[i] - base_rain:+.0f} |\n"
    
    result += f"""
---

## 4. ì§ˆí™˜ ë°œìƒ íŠ¸ë Œë“œ ì˜ˆì¸¡ (í–¥í›„ {years_ahead}ë…„)

### ì˜¨ì—´ì§ˆí™˜ ì˜ˆì¸¡

| ì—°ë„ | ì˜ˆì¸¡ ë°œìƒ ê±´ìˆ˜ | ì „ë…„ ëŒ€ë¹„ |
|------|---------------|----------|
"""
    
    for i, year in enumerate(range(2025, 2025 + years_ahead)):
        prev = heat_forecast[i-1] if i > 0 else merged['ì˜¨ì—´ì§ˆí™˜'].iloc[-1]
        change_pct = (heat_forecast[i] - prev) / prev * 100 if prev > 0 else 0
        result += f"| {year} | {heat_forecast[i]:.0f} | {change_pct:+.1f}% |\n"
    
    result += f"""
### ìˆ˜ì¸ì„±ì§ˆí™˜ ì˜ˆì¸¡

| ì—°ë„ | ì˜ˆì¸¡ ë°œìƒ ê±´ìˆ˜ | ì „ë…„ ëŒ€ë¹„ |
|------|---------------|----------|
"""
    
    for i, year in enumerate(range(2025, 2025 + years_ahead)):
        prev = water_forecast[i-1] if i > 0 else merged['ìˆ˜ì¸ì„±ì§ˆí™˜'].iloc[-1]
        change_pct = (water_forecast[i] - prev) / prev * 100 if prev > 0 else 0
        result += f"| {year} | {water_forecast[i]:.0f} | {change_pct:+.1f}% |\n"
    
    # ìµœì¢… ìš”ì•½
    temp_change = temp_forecast[-1] - base_temp
    rain_change = rain_forecast[-1] - base_rain
    heat_change = heat_forecast[-1] - merged['ì˜¨ì—´ì§ˆí™˜'].iloc[-1]
    water_change = water_forecast[-1] - merged['ìˆ˜ì¸ì„±ì§ˆí™˜'].iloc[-1]
    
    result += f"""
---

## 5. ì¢…í•© ìš”ì•½ ë° ì‹œì‚¬ì 

### {years_ahead}ë…„ í›„ ì˜ˆì¸¡ ìš”ì•½ ({2024 + years_ahead}ë…„)

| ì§€í‘œ | í˜„ì¬ (2024) | ì˜ˆì¸¡ ({2024 + years_ahead}) | ë³€í™”ëŸ‰ | ë³€í™”ìœ¨ |
|------|------------|---------------------------|--------|--------|
| í‰ê· ê¸°ì˜¨ | {base_temp:.2f}Â°C | {temp_forecast[-1]:.2f}Â°C | {temp_change:+.2f}Â°C | {temp_change/base_temp*100:+.1f}% |
| ì—°ê°•ìˆ˜ëŸ‰ | {base_rain:.0f}mm | {rain_forecast[-1]:.0f}mm | {rain_change:+.0f}mm | {rain_change/base_rain*100:+.1f}% |
| ì˜¨ì—´ì§ˆí™˜ | {merged['ì˜¨ì—´ì§ˆí™˜'].iloc[-1]:.0f}ê±´ | {heat_forecast[-1]:.0f}ê±´ | {heat_change:+.0f}ê±´ | {heat_change/merged['ì˜¨ì—´ì§ˆí™˜'].iloc[-1]*100:+.1f}% |
| ìˆ˜ì¸ì„±ì§ˆí™˜ | {merged['ìˆ˜ì¸ì„±ì§ˆí™˜'].iloc[-1]:.0f}ê±´ | {water_forecast[-1]:.0f}ê±´ | {water_change:+.0f}ê±´ | {water_change/merged['ìˆ˜ì¸ì„±ì§ˆí™˜'].iloc[-1]*100:+.1f}% |

### ë³´í—˜ê³„ë¦¬ì  ì‹œì‚¬ì 

1. **ì˜¨ì—´ì§ˆí™˜ ë¦¬ìŠ¤í¬ ì¦ê°€**: ê¸°ì˜¨ ìƒìŠ¹ìœ¼ë¡œ ì¸í•´ ì˜¨ì—´ì§ˆí™˜ ë°œìƒì´ ì§€ì†ì ìœ¼ë¡œ ì¦ê°€í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë¨
   - ê±´ê°•ë³´í—˜ ì†í•´ìœ¨ ìƒìŠ¹ ê°€ëŠ¥ì„±
   - ì—¬ë¦„ì²  ê³ ì˜¨ ê´€ë ¨ íŠ¹ì•½ ìƒí’ˆ ê°œë°œ í•„ìš”

2. **ìˆ˜ì¸ì„±ì§ˆí™˜ ë¦¬ìŠ¤í¬**: ê°•ìˆ˜ëŸ‰ ë³€ë™ì„± ì¦ê°€ë¡œ ì§‘ì¤‘í˜¸ìš° ì‹œ ìˆ˜ì¸ì„±ì§ˆí™˜ ê¸‰ì¦ ê°€ëŠ¥
   - ì¬í•´ ê´€ë ¨ ë³´í—˜ ìƒí’ˆ ë¦¬ìŠ¤í¬ ê´€ë¦¬ í•„ìš”
   - ê¸°í›„ ë³€ë™ì„±ì„ ë°˜ì˜í•œ ë³´í—˜ë£Œ ì‚°ì • ê³ ë ¤

3. **ì¥ê¸° íŠ¸ë Œë“œ**: ê¸°í›„ë³€í™”ì— ë”°ë¥¸ ì§ˆí™˜ íŒ¨í„´ ë³€í™”ë¥¼ ë³´í—˜ ìƒí’ˆ ì„¤ê³„ì— ë°˜ì˜ í•„ìš”
"""
    
    return result

# ============== TEXT-TO-SQL TOOLS (PostgreSQL + Neo4j Schema) ==============

@tool
def search_table_schema(query: str, top_k: int = 5) -> str:
    """
    Searches for relevant database tables/views and their columns in Neo4j using vector similarity.
    The schema is stored in Neo4j with:
    - ObjectType nodes: represent tables/views with 'name' property
    - Column nodes: represent columns, connected via HAS_COLUMN relationship
    - Column nodes have 'vector' embeddings for semantic search
    
    Args:
        query: Natural language description of what data you're looking for
        top_k: Number of relevant tables to return (default: 5)
    
    Returns:
        Schema information for relevant tables including column details
    """
    if not driver:
        return "Neo4j database connection not available."
    
    try:
        query_embedding = get_embedding(query)
        
        with driver.session() as session:
            # First, search for relevant Column nodes using vector similarity
            # Then get their parent ObjectType tables
            result = session.run("""
                MATCH (t:ObjectType)-[:HAS_COLUMN]->(c:Column)
                WHERE c.vector IS NOT NULL
                WITH t, c, c.vector AS col_embedding
                RETURN t.name AS table_name,
                       t.description AS table_description,
                       t.query AS table_query,
                       t.schema AS table_schema,
                       collect({
                           name: c.name,
                           dtype: c.dtype,
                           description: c.description,
                           nullable: c.nullable,
                           vector: c.vector
                       }) AS columns
            """)
            
            records = list(result)
            
            if not records:
                # Fallback: get all ObjectType nodes even without embeddings
                result = session.run("""
                    MATCH (t:ObjectType)
                    OPTIONAL MATCH (t)-[:HAS_COLUMN]->(c:Column)
                    WITH t, collect({
                        name: c.name,
                        dtype: c.dtype,
                        description: c.description,
                        nullable: c.nullable
                    }) AS columns
                    RETURN t.name AS table_name,
                           t.description AS table_description,
                           t.query AS table_query,
                           t.schema AS table_schema,
                           columns
                """)
                records = list(result)
            
            if not records:
                return "No ObjectType (table/view) nodes found in the database. Please ensure the schema is loaded."
            
            # Calculate similarities based on column embeddings
            scored = []
            for record in records:
                columns = record["columns"]
                
                # Calculate max similarity across all columns
                max_sim = 0.0
                for col in columns:
                    col_vector = col.get("vector")
                    if col_vector:
                        sim = cosine_similarity(query_embedding, col_vector)
                        max_sim = max(max_sim, sim)
                
                # Also check if table name matches query keywords
                table_name = record["table_name"] or ""
                if any(keyword in table_name for keyword in query.split()):
                    max_sim = max(max_sim, 0.8)  # Boost for name match
                
                scored.append({
                    "table_name": table_name,
                    "description": record["table_description"],
                    "query": record["table_query"],
                    "schema": record["table_schema"],
                    "similarity": max_sim,
                    "columns": [c for c in columns if c.get("name")]
                })
            
            # Sort by similarity
            scored.sort(key=lambda x: x["similarity"], reverse=True)
            top_results = scored[:top_k]
            
            if not top_results:
                return "No matching tables found for the query."
            
            # Format output
            output = f"## ğŸ” ê´€ë ¨ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ {len(top_results)}ê°œ)\n\n"
            
            for i, table in enumerate(top_results, 1):
                output += f"### {i}. `{table['table_name']}` (ìœ ì‚¬ë„: {table['similarity']:.3f})\n"
                if table['description']:
                    output += f"**ì„¤ëª…**: {table['description']}\n\n"
                if table['schema']:
                    output += f"**ìŠ¤í‚¤ë§ˆ**: {table['schema']}\n"
                if table['query']:
                    output += f"**ì›ë³¸ ì¿¼ë¦¬**:\n```sql\n{table['query']}\n```\n\n"
                
                output += "**ì»¬ëŸ¼ ëª©ë¡**:\n"
                output += "| ì»¬ëŸ¼ëª… | íƒ€ì… | ì„¤ëª… | Nullable |\n"
                output += "|--------|------|------|----------|\n"
                
                for col in table['columns']:
                    nullable = "âœ“" if col.get('nullable') else "âœ—"
                    col_desc = col.get('description', '') or ''
                    col_type = col.get('dtype', 'unknown') or 'unknown'
                    output += f"| {col['name']} | {col_type} | {col_desc} | {nullable} |\n"
                
                output += "\n"
            
            return output
    except Exception as e:
        return f"Schema search error: {e}"


@tool
def text_to_sql(question: str) -> str:
    """
    Converts a natural language question to SQL and executes it against PostgreSQL (meetingroom DB).
    
    This tool:
    1. Searches Neo4j for relevant table schemas using vector similarity
    2. Generates SQL query using GPT-4
    3. Executes the SQL against PostgreSQL
    4. Returns the results
    
    Args:
        question: Natural language question about the data (e.g., "ìƒí’ˆëª©ë¡ ë³´ì—¬ì¤˜", "ë³´í—˜ê°€ì…ë‚´ì—­ ì¡°íšŒ")
    
    Returns:
        SQL query results with the generated SQL shown for transparency
    """
    if not driver:
        return "Neo4j database connection not available for schema lookup."
    
    try:
        # Step 1: Search for relevant schema in Neo4j
        query_embedding = get_embedding(question)
        
        schema_info = ""
        top_tables = []
        with driver.session() as session:
            # Get all ObjectType nodes with their columns
            result = session.run("""
                MATCH (t:ObjectType)
                OPTIONAL MATCH (t)-[:HAS_COLUMN]->(c:Column)
                WITH t, collect({
                    name: c.name,
                    dtype: c.dtype,
                    description: c.description,
                    vector: c.vector
                }) AS columns
                RETURN t.name AS table_name,
                       t.description AS table_description,
                       t.query AS table_query,
                       t.schema AS table_schema,
                       columns
            """)
            
            records = list(result)
            
            if not records:
                return "No tables found in Neo4j. Please check if schema is loaded."
            
            # Calculate similarities based on column embeddings and table name
            scored = []
            for record in records:
                columns = record["columns"]
                table_name = record["table_name"] or ""
                
                # Calculate max similarity across all columns
                max_sim = 0.0
                for col in columns:
                    col_vector = col.get("vector")
                    if col_vector:
                        sim = cosine_similarity(query_embedding, col_vector)
                        max_sim = max(max_sim, sim)
                
                # Boost score if table name appears in question
                if table_name and table_name in question:
                    max_sim = max(max_sim, 0.95)  # High boost for exact match
                elif table_name and any(keyword in table_name for keyword in question.split()):
                    max_sim = max(max_sim, 0.8)  # Boost for partial match
                
                scored.append({
                    "table_name": table_name,
                    "description": record["table_description"],
                    "query": record["table_query"],
                    "schema": record["table_schema"],
                    "similarity": max_sim,
                    "columns": [c for c in columns if c.get("name")]
                })
            
            scored.sort(key=lambda x: x["similarity"], reverse=True)
            top_tables = scored[:5]
            
            if not top_tables:
                return "No matching tables found. Please check if schema is loaded in Neo4j."
            
            # Build schema context for GPT
            for table in top_tables:
                table_name = table['table_name']
                schema_info += f"\nTable/View: {table_name}"
                if table['description']:
                    schema_info += f" -- {table['description']}"
                if table['query']:
                    schema_info += f"\n  Original Query: {table['query']}"
                schema_info += "\nColumns:\n"
                for col in table['columns']:
                    col_desc = f" -- {col.get('description', '')}" if col.get('description') else ""
                    col_dtype = col.get('dtype', 'unknown') or 'unknown'
                    schema_info += f"  - {col['name']} ({col_dtype}){col_desc}\n"
        
        # Step 2: Generate SQL using GPT-4
        sql_prompt = f"""You are a SQL expert. Based on the following PostgreSQL schema, write a SQL query to answer the user's question.

## Available Tables Schema:
{schema_info}

## User Question:
{question}

## Instructions:
1. Write a valid PostgreSQL query
2. Use proper JOINs if multiple tables are needed
3. Add appropriate WHERE clauses for filtering
4. Use meaningful column aliases in Korean if appropriate
5. Limit results to 100 rows unless the user asks for more
6. Return ONLY the SQL query, no explanation

## SQL Query:"""

        response = openai_client.chat.completions.create(
            model="gpt-4.1",
            messages=[{"role": "user", "content": sql_prompt}],
            max_tokens=1000,
            temperature=0
        )
        
        generated_sql = response.choices[0].message.content.strip()
        
        # Clean up the SQL (remove markdown if present)
        if "```sql" in generated_sql:
            generated_sql = generated_sql.split("```sql")[1].split("```")[0].strip()
        elif "```" in generated_sql:
            generated_sql = generated_sql.split("```")[1].split("```")[0].strip()
        
        # Step 3: Execute SQL against PostgreSQL
        conn = get_postgres_connection()
        if not conn:
            return f"""## SQL ìƒì„± ì™„ë£Œ (ì‹¤í–‰ ì‹¤íŒ¨)

**ìƒì„±ëœ SQL**:
```sql
{generated_sql}
```

**ì˜¤ë¥˜**: PostgreSQL ì—°ê²° ì‹¤íŒ¨. ë°ì´í„°ë² ì´ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.
- Host: {POSTGRES_HOST}
- Port: {POSTGRES_PORT}
- Database: {POSTGRES_DB}
"""
        
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute(generated_sql)
                
                # Check if it's a SELECT query
                if generated_sql.strip().upper().startswith("SELECT"):
                    rows = cursor.fetchall()
                    
                    if not rows:
                        result_text = "ì¡°íšŒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
                    else:
                        # Convert to markdown table format (limit to 25 rows)
                        import pandas as pd
                        df = pd.DataFrame(rows)
                        total_rows = len(df)
                        display_limit = 25
                        
                        if total_rows > display_limit:
                            display_df = df.head(display_limit)
                            result_text = f"**ì´ {total_rows}ê±´** ì¤‘ ìƒìœ„ {display_limit}ê±´ì„ í‘œì‹œí•©ë‹ˆë‹¤.\n\n"
                        else:
                            display_df = df
                            result_text = f"**ì¡°íšŒ ê²°ê³¼**: {total_rows}ê±´\n\n"
                        
                        # Convert to markdown table
                        result_text += display_df.to_markdown(index=False)
                        
                        if total_rows > display_limit:
                            result_text += f"\n\n> ğŸ’¡ ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•˜ì‹œë©´ ì¡°ê±´ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”."
                else:
                    conn.commit()
                    result_text = f"ì¿¼ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì˜í–¥ë°›ì€ í–‰: {cursor.rowcount}"
                
        except Exception as sql_error:
            conn.rollback()
            return f"""## SQL ì‹¤í–‰ ì˜¤ë¥˜

**ìƒì„±ëœ SQL**:
```sql
{generated_sql}
```

**ì˜¤ë¥˜**: {sql_error}

ìŠ¤í‚¤ë§ˆë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì„¸ìš”.
"""
        finally:
            conn.close()
        
        return f"""## ğŸ“Š Text-to-SQL ì‹¤í–‰ ê²°ê³¼

### ì§ˆë¬¸
{question}

### ìƒì„±ëœ SQL
```sql
{generated_sql}
```

### ê²°ê³¼
{result_text}

### ì‚¬ìš©ëœ í…Œì´ë¸”
{', '.join([t['table_name'] for t in top_tables[:3]])}
"""
    
    except Exception as e:
        return f"Text-to-SQL error: {e}"


@tool
def run_postgres_sql(sql_query: str) -> str:
    """
    Executes a raw SQL query against the PostgreSQL database (meetingroom DB).
    Use this for direct SQL execution when you already know the exact query.
    
    CAUTION: This tool executes SQL directly. Be careful with DELETE/UPDATE/DROP statements.
    
    Args:
        sql_query: The SQL query to execute
    
    Returns:
        Query results or execution status
    """
    conn = get_postgres_connection()
    if not conn:
        return f"""## PostgreSQL ì—°ê²° ì‹¤íŒ¨

ì—°ê²° ì •ë³´:
- Host: {POSTGRES_HOST}
- Port: {POSTGRES_PORT}
- Database: {POSTGRES_DB}
- User: {POSTGRES_USER}

ë°ì´í„°ë² ì´ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.
"""
    
    try:
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute(sql_query)
            
            # Check if it's a SELECT query
            if sql_query.strip().upper().startswith("SELECT"):
                rows = cursor.fetchall()
                
                if not rows:
                    return f"""## SQL ì‹¤í–‰ ê²°ê³¼

```sql
{sql_query}
```

ì¡°íšŒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.
"""
                
                # Convert to markdown table format (limit to 25 rows)
                import pandas as pd
                df = pd.DataFrame(rows)
                total_rows = len(df)
                display_limit = 25
                
                if total_rows > display_limit:
                    display_df = df.head(display_limit)
                    count_msg = f"**ì´ {total_rows}ê±´** ì¤‘ ìƒìœ„ {display_limit}ê±´ì„ í‘œì‹œí•©ë‹ˆë‹¤."
                    footer_msg = f"\n\n> ğŸ’¡ ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•˜ì‹œë©´ ì¡°ê±´ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”."
                else:
                    display_df = df
                    count_msg = f"**ì¡°íšŒ ê²°ê³¼**: {total_rows}ê±´"
                    footer_msg = ""
                
                # Convert to markdown table
                markdown_table = display_df.to_markdown(index=False)
                
                return f"""## ğŸ“‹ SQL ì‹¤í–‰ ê²°ê³¼

```sql
{sql_query}
```

{count_msg}

{markdown_table}

**ì»¬ëŸ¼**: {', '.join(df.columns.tolist())}{footer_msg}
"""
            else:
                conn.commit()
                return f"""## SQL ì‹¤í–‰ ê²°ê³¼

```sql
{sql_query}
```

ì¿¼ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.
- ì˜í–¥ë°›ì€ í–‰: {cursor.rowcount}
"""
    
    except Exception as e:
        conn.rollback()
        return f"""## SQL ì‹¤í–‰ ì˜¤ë¥˜

```sql
{sql_query}
```

**ì˜¤ë¥˜**: {e}
"""
    finally:
        conn.close()


@tool
def get_postgres_tables() -> str:
    """
    Lists all tables in the PostgreSQL database (meetingroom DB).
    Use this to see what tables are available for querying.
    
    Returns:
        List of all tables with their schema and row counts
    """
    conn = get_postgres_connection()
    if not conn:
        return f"PostgreSQL ì—°ê²° ì‹¤íŒ¨. Host: {POSTGRES_HOST}, DB: {POSTGRES_DB}"
    
    try:
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # Get all tables with row counts
            cursor.execute("""
                SELECT 
                    schemaname,
                    tablename,
                    (SELECT count(*) FROM information_schema.columns 
                     WHERE table_schema = t.schemaname AND table_name = t.tablename) as column_count
                FROM pg_catalog.pg_tables t
                WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
                ORDER BY schemaname, tablename
            """)
            
            tables = cursor.fetchall()
            
            if not tables:
                return "ë°ì´í„°ë² ì´ìŠ¤ì— í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤."
            
            output = f"""## ğŸ“‹ PostgreSQL í…Œì´ë¸” ëª©ë¡

**ë°ì´í„°ë² ì´ìŠ¤**: {POSTGRES_DB}
**ì´ í…Œì´ë¸” ìˆ˜**: {len(tables)}

| ìŠ¤í‚¤ë§ˆ | í…Œì´ë¸”ëª… | ì»¬ëŸ¼ ìˆ˜ |
|--------|----------|---------|
"""
            
            for table in tables:
                output += f"| {table['schemaname']} | {table['tablename']} | {table['column_count']} |\n"
            
            output += """
---

*ìƒì„¸ ìŠ¤í‚¤ë§ˆë¥¼ ë³´ë ¤ë©´ `search_table_schema("í…Œì´ë¸” ì„¤ëª…")` ë˜ëŠ” `run_postgres_sql("SELECT * FROM table_name LIMIT 5")` ì‚¬ìš©*
"""
            
            return output
    
    except Exception as e:
        return f"í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}"
    finally:
        conn.close()


# ============== ARTIFACT MANAGEMENT TOOLS ==============

@tool
def save_artifact(name: str, description: str, content: str, artifact_type: str = "analysis_result", source_url: str = "") -> str:
    """
    Saves an artifact (search result, analysis, downloaded data) to Neo4j for future retrieval.
    The artifact is vectorized and stored with the current session ID for priority-based search.
    
    Args:
        name: Short name/title of the artifact
        description: Description of what this artifact contains
        content: The actual content (text, CSV data, analysis result, etc.)
        artifact_type: Type of artifact - "search_result", "downloaded_file", "analysis_result", "csv_data"
        source_url: Source URL if applicable
    
    Returns:
        Confirmation message with artifact ID
    """
    if not driver:
        return "Database connection not available."
    
    import uuid
    from datetime import datetime
    
    try:
        session_id = get_session_id()
        artifact_id = f"ART_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{str(uuid.uuid4())[:6]}"
        
        # Create embedding from name + description + content (truncated)
        embed_text = f"{name}: {description}\n{content[:3000]}"
        embedding = get_embedding(embed_text)
        
        with driver.session() as db_session:
            # Create or get Session node
            db_session.run("""
                MERGE (s:Session {id: $session_id})
                ON CREATE SET s.created_at = datetime()
            """, session_id=session_id)
            
            # Create Artifact node
            db_session.run("""
                CREATE (a:Artifact {
                    id: $artifact_id,
                    name: $name,
                    description: $description,
                    content: $content,
                    artifact_type: $artifact_type,
                    source_url: $source_url,
                    session_id: $session_id,
                    created_at: datetime(),
                    embedding: $embedding
                })
                WITH a
                MATCH (s:Session {id: $session_id})
                MERGE (a)-[:BELONGS_TO_SESSION]->(s)
            """, artifact_id=artifact_id, name=name, description=description,
                content=content[:50000], artifact_type=artifact_type, 
                source_url=source_url, session_id=session_id, embedding=embedding)
        
        return f"""âœ… **ì‚°ì¶œë¬¼ ì €ì¥ ì™„ë£Œ**
- **ID**: {artifact_id}
- **ì´ë¦„**: {name}
- **ìœ í˜•**: {artifact_type}
- **ì„¸ì…˜**: {session_id}
- **ë‚´ìš© í¬ê¸°**: {len(content)} ë¬¸ì

ì´ ì‚°ì¶œë¬¼ì€ `search_artifacts` ë„êµ¬ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
    except Exception as e:
        return f"Artifact save error: {e}"


@tool
def search_artifacts(query: str, top_k: int = 5, current_session_only: bool = True) -> str:
    """
    Searches for previously saved artifacts using semantic similarity.
    By default, only searches within the current session.
    
    Args:
        query: Search query describing what you're looking for
        top_k: Number of results to return (default: 5)
        current_session_only: If True (default), only search within current session.
                              Set to False to search all sessions.
    
    Returns:
        List of matching artifacts with their content
    """
    if not driver:
        return "Database connection not available."
    
    try:
        session_id = get_session_id()
        query_embedding = get_embedding(query)
        
        with driver.session() as db_session:
            if current_session_only:
                result = db_session.run("""
                    MATCH (a:Artifact)
                    WHERE a.session_id = $session_id AND a.embedding IS NOT NULL
                    RETURN a.id AS id, a.name AS name, a.description AS description, 
                           a.content AS content, a.artifact_type AS type,
                           a.source_url AS source_url, a.session_id AS session_id,
                           a.embedding AS embedding
                """, session_id=session_id)
            else:
                result = db_session.run("""
                    MATCH (a:Artifact)
                    WHERE a.embedding IS NOT NULL
                    RETURN a.id AS id, a.name AS name, a.description AS description, 
                           a.content AS content, a.artifact_type AS type,
                           a.source_url AS source_url, a.session_id AS session_id,
                           a.embedding AS embedding
                """)
            
            records = list(result)
            
            if not records:
                return "ì €ì¥ëœ ì‚°ì¶œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤."
            
            # Calculate similarities with session priority boost
            scored = []
            for record in records:
                artifact_embedding = record["embedding"]
                if artifact_embedding:
                    base_sim = cosine_similarity(query_embedding, artifact_embedding)
                    
                    # Boost score for current session artifacts (1.2x multiplier)
                    if record["session_id"] == session_id:
                        boosted_sim = base_sim * 1.2
                        is_current_session = True
                    else:
                        boosted_sim = base_sim
                        is_current_session = False
                    
                    scored.append({
                        "id": record["id"],
                        "name": record["name"],
                        "description": record["description"],
                        "content": record["content"][:2000] if record["content"] else "",
                        "type": record["type"],
                        "source_url": record["source_url"],
                        "session_id": record["session_id"],
                        "similarity": boosted_sim,
                        "is_current_session": is_current_session
                    })
            
            # Sort by boosted similarity
            scored.sort(key=lambda x: x["similarity"], reverse=True)
            top_results = scored[:top_k]
            
            # Format output
            output_parts = [f"## ğŸ” ì‚°ì¶œë¬¼ ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ {len(top_results)}ê°œ)\n"]
            output_parts.append(f"**í˜„ì¬ ì„¸ì…˜**: {session_id}\n")
            
            for i, item in enumerate(top_results, 1):
                session_badge = "ğŸŸ¢ í˜„ì¬ ì„¸ì…˜" if item["is_current_session"] else "âšª ì´ì „ ì„¸ì…˜"
                output_parts.append(f"""
### {i}. {item['name']} [{session_badge}]
- **ID**: {item['id']}
- **ìœ í˜•**: {item['type']}
- **ìœ ì‚¬ë„**: {item['similarity']:.3f}
- **ì„¤ëª…**: {item['description']}

**ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°**:
```
{item['content'][:1000]}{'...' if len(item['content']) > 1000 else ''}
```
""")
            
            return "\n".join(output_parts)
    except Exception as e:
        return f"Artifact search error: {e}"


@tool
def get_artifact_content(artifact_id: str) -> str:
    """
    Retrieves the full content of a specific artifact by its ID.
    
    Args:
        artifact_id: The ID of the artifact to retrieve
    
    Returns:
        Full artifact content
    """
    if not driver:
        return "Database connection not available."
    
    try:
        with driver.session() as db_session:
            result = db_session.run("""
                MATCH (a:Artifact {id: $artifact_id})
                RETURN a.name AS name, a.description AS description, 
                       a.content AS content, a.artifact_type AS type,
                       a.source_url AS source_url, a.session_id AS session_id,
                       a.created_at AS created_at
            """, artifact_id=artifact_id)
            
            record = result.single()
            
            if not record:
                return f"ì‚°ì¶œë¬¼ '{artifact_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            return f"""## ğŸ“„ ì‚°ì¶œë¬¼: {record['name']}

**ID**: {artifact_id}
**ìœ í˜•**: {record['type']}
**ì„¸ì…˜**: {record['session_id']}
**ì†ŒìŠ¤**: {record['source_url'] or 'N/A'}

### ì„¤ëª…
{record['description']}

### ì „ì²´ ë‚´ìš©
```
{record['content']}
```
"""
    except Exception as e:
        return f"Artifact retrieval error: {e}"


@tool
def list_session_artifacts(session_id: str = "") -> str:
    """
    Lists all artifacts in a specific session or the current session.
    
    Args:
        session_id: Session ID to list artifacts for (empty = current session)
    
    Returns:
        List of artifacts in the session
    """
    if not driver:
        return "Database connection not available."
    
    try:
        target_session = session_id if session_id else get_session_id()
        
        with driver.session() as db_session:
            result = db_session.run("""
                MATCH (a:Artifact)
                WHERE a.session_id = $session_id
                RETURN a.id AS id, a.name AS name, a.artifact_type AS type,
                       a.description AS description, a.created_at AS created_at
                ORDER BY a.created_at DESC
            """, session_id=target_session)
            
            records = list(result)
            
            if not records:
                return f"ì„¸ì…˜ '{target_session}'ì— ì €ì¥ëœ ì‚°ì¶œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤."
            
            output_parts = [f"## ğŸ“‹ ì„¸ì…˜ '{target_session}' ì‚°ì¶œë¬¼ ëª©ë¡\n"]
            output_parts.append(f"ì´ {len(records)}ê°œ ì‚°ì¶œë¬¼\n")
            
            for i, record in enumerate(records, 1):
                output_parts.append(f"""
{i}. **{record['name']}** [{record['type']}]
   - ID: `{record['id']}`
   - ì„¤ëª…: {record['description'][:100]}{'...' if len(record['description'] or '') > 100 else ''}
""")
            
            return "\n".join(output_parts)
    except Exception as e:
        return f"Error listing artifacts: {e}"


class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]

# LLM - gpt-4.1 supports up to 1M tokens context (vs gpt-4o's 128K limit)
# Enable streaming for real-time response
llm = ChatOpenAI(model="gpt-4.1", api_key=OPENAI_API_KEY, temperature=0, streaming=True)
tools = [similarity_search, run_cypher, get_formula_details, calculate_formula, web_search, fetch_webpage, download_file, read_downloaded_file, list_downloaded_files, summarize_document, execute_python_code, extract_data_to_csv, run_correlation_analysis, run_forecast_analysis, forecast_climate_trend, forecast_disease_trend, forecast_holt_winters, forecast_comprehensive_analysis, search_table_schema, text_to_sql, run_postgres_sql, get_postgres_tables, save_artifact, search_artifacts, get_artifact_content, list_session_artifacts]
llm_with_tools = llm.bind_tools(tools)

def agent_node(state: AgentState):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

# Graph Construction
workflow = StateGraph(AgentState)
workflow.add_node("agent", agent_node)
workflow.add_node("tools", ToolNode(tools))

workflow.set_entry_point("agent")
workflow.add_conditional_edges("agent", tools_condition)
workflow.add_edge("tools", "agent")

graph = workflow.compile()

SYSTEM_PROMPT = f"""You are an Actuarial Assistant for Korean insurance documents.
You have access to a Neo4j knowledge graph with embedded vectors for semantic search.

{SCHEMA_INFO}

## Available Tools:
1. **similarity_search(query, node_type, top_k)**: Semantic search using embeddings. 
   - Use this FIRST to find relevant formulas/concepts.
   - node_type can be "Formula", "Concept", "Variable", "Definition", or "all"

2. **get_formula_details(formula_name)**: Get complete formula info including LaTeX and all variables.

3. **run_cypher(query)**: Execute Cypher queries for specific graph traversals.

4. **calculate_formula(expression, variables)**: Evaluate a formula with given values.

5. **web_search(query, max_results)**: Search the web using Tavily API.
   - Use for real-time information: latest regulations, market data, news
   - Use when knowledge graph doesn't have the needed information
   - Supports Korean and English queries

6. **fetch_webpage(url)**: Fetch and extract text content from a webpage.
   - Use to read full content of a specific URL

7. **download_file(url, filename)**: Download a file from URL (PDF, Excel, CSV, etc.).
   - Saves to local downloads folder
   - Returns filepath for subsequent reading

8. **read_downloaded_file(filepath, max_chars)**: Read content from downloaded files.
   - Supports PDF, Excel (.xlsx), CSV, and text files
   - Use after download_file to extract content

9. **list_downloaded_files()**: List all files in downloads folder.

10. **summarize_document(filepath, focus_topic)**: Summarize a document using AI.
    - Extracts key findings, data tables, and provides structured summary
    - focus_topic: Optional topic to focus on (e.g., "ê°•ìˆ˜ëŸ‰", "ì˜¨ì—´ì§ˆí™˜")
    - Auto-saves summary as Artifact

11. **execute_python_code(code)**: Execute Python code for data analysis.
    - Has access to pandas, numpy, scipy.stats, matplotlib
    - Can read files from downloads folder using DOWNLOAD_DIR
    - Use for custom calculations, data processing, visualization

11. **extract_data_to_csv(source_description)**: Extract data and save as CSV.
    - Creates reusable dataset from downloaded files
    - Returns CSV file path for subsequent analysis
    - Use FIRST before running analysis

12. **run_correlation_analysis(csv_file)**: Run correlation analysis on CSV data.
    - Reads actual data from CSV file
    - Returns results WITH Python code used (for transparency)
    - Shows data source and methodology

13. **run_forecast_analysis(csv_file, years_ahead)**: Run forecast analysis on CSV data.
    - Polynomial regression for climate, multiple regression for disease
    - Returns results WITH Python code used (for transparency)
    - Shows predictions with confidence

14. **forecast_climate_trend(years_ahead)**: Forecast climate trends (temperature, precipitation).
    - Uses polynomial regression for climate change trend
    - Based on KMA (Korea Meteorological Administration) data 2000-2024

13. **forecast_disease_trend(years_ahead)**: Forecast disease trends (heat illness, waterborne).
    - Uses multivariate regression with climate variables
    - Based on KDCA (Korea Disease Control) data 2010-2024

14. **forecast_holt_winters(data_type, years_ahead)**: Holt-Winters time series forecasting.
    - data_type: "temperature", "precipitation", "heat_illness", "waterborne"
    - Provides confidence intervals

15. **forecast_comprehensive_analysis(years_ahead)**: Complete climate-disease trend analysis.
    - Combines all forecasting methods
    - Use this for comprehensive 10-year trend analysis
    - Includes correlation, climate forecast, disease forecast, and actuarial implications

## Text-to-SQL Tools (PostgreSQL Database - meetingroom):
16. **search_table_schema(query, top_k)**: Search for relevant tables in Neo4j using vector similarity.
    - Schema is stored as ObjectType (tables) and Column (fields) nodes in Neo4j
    - Use this FIRST to understand what tables are available
    - Returns table descriptions, columns, types, and descriptions

17. **text_to_sql(question)**: Convert natural language to SQL and execute against PostgreSQL.
    - Automatically finds relevant schemas from Neo4j
    - Generates SQL using GPT-4
    - Executes against meetingroom database
    - Returns results with the generated SQL for transparency
    - Example: "ì˜¤ëŠ˜ ì˜ˆì•½ëœ íšŒì˜ì‹¤ ëª©ë¡", "ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” íšŒì˜ì‹¤"

18. **run_postgres_sql(sql_query)**: Execute raw SQL directly against PostgreSQL.
    - Use when you know the exact SQL to execute
    - Supports SELECT, INSERT, UPDATE, DELETE
    - Be careful with data modification queries

19. **get_postgres_tables()**: List all tables in the PostgreSQL database.
    - Shows schema, table names, and column counts
    - Use to explore what's available in the database

## Artifact Management Tools (ì‚°ì¶œë¬¼ ê´€ë¦¬):
20. **save_artifact(name, description, content, artifact_type, source_url)**: Save analysis results for future use.
    - artifact_type: "search_result", "downloaded_file", "analysis_result", "csv_data"
    - Stored with session ID and vectorized for semantic search
    - **IMPORTANT**: Save important results after web_search, analysis, or data extraction

21. **search_artifacts(query, top_k, current_session_only=True)**: Search previously saved artifacts.
    - By default, searches ONLY within current session (current_session_only=True)
    - Set current_session_only=False to search across all sessions
    - Use to retrieve data you already found/analyzed

22. **get_artifact_content(artifact_id)**: Get full content of a specific artifact by ID.

23. **list_session_artifacts(session_id)**: List all artifacts in a session.
    - Empty session_id = current session

## Instructions:

### âš ï¸ ê²€ìƒ‰ ìš°ì„ ìˆœìœ„ (ë§¤ìš° ì¤‘ìš” - ë°˜ë“œì‹œ ì¤€ìˆ˜):
**ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ â†’ ì™¸ë¶€ ê²€ìƒ‰ ìˆœì„œë¡œ ì§„í–‰í•´ì•¼ í•©ë‹ˆë‹¤!**

1. **"~ëª©ë¡ ë³´ì—¬ì¤˜", "~ì¡°íšŒí•´ì¤˜", "~ë°ì´í„° ë³´ì—¬ì¤˜" ìš”ì²­ ì‹œ**:
   - **FIRST**: `text_to_sql` ë˜ëŠ” `search_table_schema` ì‚¬ìš© â†’ ë‚´ë¶€ PostgreSQL DB ê²€ìƒ‰
   - ì˜ˆ: "ìƒí’ˆëª©ë¡ ë³´ì—¬ì¤˜" â†’ `text_to_sql("ìƒí’ˆëª©ë¡ ë³´ì—¬ì¤˜")` ë¨¼ì € ì‹¤í–‰
   - ì˜ˆ: "ë³´í—˜ê°€ì…ë‚´ì—­ ì¡°íšŒ" â†’ `text_to_sql("ë³´í—˜ê°€ì…ë‚´ì—­ ì¡°íšŒ")` ë¨¼ì € ì‹¤í–‰
   - **ONLY IF** ë‚´ë¶€ DBì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ â†’ `web_search` ì‚¬ìš©

2. **For questions about formulas or concepts**:
   - Use `similarity_search` FIRST to find in knowledge graph.
   - Then use `get_formula_details` to get the full LaTeX and variables.

3. When calculating, extract the 'expression' field from the formula and use calculate_formula.
4. For current market data, regulations, or external information ONLY, use web_search.
5. To get detailed webpage content, use fetch_webpage with the URL.

## âš ï¸ ìë™ íŒŒì‹± ë° ìš”ì•½ ì›Œí¬í”Œë¡œìš° (CRITICAL - ë°˜ë“œì‹œ ë”°ë¥¼ ê²ƒ):

### ğŸ“¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œ ìë™ íŒŒì‹±:
When you download a file (PDF, Excel, etc.), you MUST IMMEDIATELY:
1. Call `download_file(url)` to save the file
2. Call `read_downloaded_file(filepath)` to parse the content
3. Call `summarize_document(filepath, focus_topic)` to create a summary
4. All results are auto-saved as Artifacts

### ğŸ“ "~ëª©ë¡ ë³´ì—¬ì¤˜", "~ì¡°íšŒí•´ì¤˜", "ë°ì´í„° ë³´ì—¬ì¤˜" ìš”ì²­ ì²˜ë¦¬ (ë°ì´í„° ì¡°íšŒ):
When user asks for data listing or query:
- **text_to_sqlë§Œ í˜¸ì¶œí•˜ì„¸ìš”!** ë‹¤ë¥¸ ë„êµ¬ëŠ” í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”.
- text_to_sqlì´ Neo4j ìŠ¤í‚¤ë§ˆ ê²€ìƒ‰ê³¼ PostgreSQL ì¿¼ë¦¬ë¥¼ ëª¨ë‘ ì²˜ë¦¬í•©ë‹ˆë‹¤.
- ê²°ê³¼ê°€ ì—†ì„ ë•Œë§Œ web_searchë¥¼ ê³ ë ¤í•˜ì„¸ìš”.

âš ï¸ **ì¤‘ë³µ í˜¸ì¶œ ê¸ˆì§€**: search_table_schema, search_artifacts, similarity_search ë“±ì„ 
   ë°ì´í„° ì¡°íšŒ ì‹œ í•¨ê»˜ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”. text_to_sql í•˜ë‚˜ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤!

### ğŸ“Š ë°ì´í„° ì¡°íšŒ ê²°ê³¼ ì¶œë ¥ í˜•ì‹:
text_to_sql ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì£¼ë©´ ë©ë‹ˆë‹¤.
í•„ìš”ì‹œ ê°„ë‹¨í•œ ì„¤ëª…ì„ ì¶”ê°€í•˜ì„¸ìš”.

### ğŸ“ "ì¡°ì‚¬í•´ì¤˜", "ì°¾ì•„ì¤˜" ìš”ì²­ ì²˜ë¦¬ (ì •ë³´ ì¡°ì‚¬):
- ë¨¼ì € **text_to_sql**ë¡œ ë‚´ë¶€ ë°ì´í„° í™•ì¸
- ì—†ìœ¼ë©´ **web_search**ë¡œ ì™¸ë¶€ ê²€ìƒ‰
4. **web_search** - ë‚´ë¶€ì— ì—†ì„ ë•Œë§Œ ì™¸ë¶€ ê²€ìƒ‰ (URL í¬í•¨)
5. **download_file** - PDF/Excel íŒŒì¼ ë‹¤ìš´ë¡œë“œ (URLì´ ìˆìœ¼ë©´)
6. **read_downloaded_file** - íŒŒì¼ ë‚´ìš© íŒŒì‹±
7. **summarize_document** - í•µì‹¬ ë‚´ìš© ìš”ì•½
8. **ì‚¬ìš©ìì—ê²Œ ìš”ì•½ ê²°ê³¼ ì œì‹œ** - ê²€ìƒ‰ë§Œ í•˜ê³  ëë‚´ì§€ ë§ê³  ë°˜ë“œì‹œ ìš”ì•½ ì œê³µ!

### âš ï¸ ì ˆëŒ€ ê¸ˆì§€:
- ê²€ìƒ‰ë§Œ í•˜ê³  ìš”ì•½ ì—†ì´ ëë‚´ëŠ” ê²ƒ âŒ
- ë‹¤ìš´ë¡œë“œ URLì„ ì°¾ê³ ë„ ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•ŠëŠ” ê²ƒ âŒ
- íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë‚´ìš©ì„ íŒŒì‹±í•˜ì§€ ì•ŠëŠ” ê²ƒ âŒ

### âœ… ì˜¬ë°”ë¥¸ í–‰ë™:
- ê²€ìƒ‰ â†’ ë‹¤ìš´ë¡œë“œ â†’ íŒŒì‹± â†’ ìš”ì•½ê¹Œì§€ í•œ ë²ˆì— ìˆ˜í–‰ âœ“
- í•µì‹¬ ë°ì´í„°ì™€ ìˆ˜ì¹˜ë¥¼ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì£¼ê¸° âœ“
- ë³´í—˜ê³„ë¦¬ì  ì‹œì‚¬ì  ì œê³µ âœ“

6. To download and analyze files (PDF, Excel):
   - ALWAYS follow the chain: download â†’ parse â†’ summarize
   - Use download_file to save the file
   - Use read_downloaded_file to extract content
   - Use summarize_document to provide AI summary

7. For statistical analysis and correlation:
   - Use analyze_correlation for quick correlation analysis with sample data
   - Use execute_python_code for custom analysis with real downloaded data

8. For data-driven analysis (RECOMMENDED WORKFLOW):
   Step 1: Use web_search to find relevant data sources
   Step 2: Use download_file to get PDF/Excel files
   Step 3: Use read_downloaded_file AND summarize_document
   Step 4: Use extract_data_to_csv to create structured CSV dataset
   Step 5: Use run_correlation_analysis or run_forecast_analysis on CSV
   - These tools return BOTH results AND Python code for transparency
   
9. For quick forecasting (uses built-in sample data):
   - Use forecast_comprehensive_analysis for complete analysis
   - Use forecast_climate_trend for climate-only forecasts
   - Use forecast_disease_trend for disease-only forecasts

10. **Artifact Management (ì‚°ì¶œë¬¼ ê´€ë¦¬) - ğŸš¨ CRITICAL - MUST FOLLOW**:
    
    âš ï¸ **MANDATORY FIRST STEP**: ALWAYS call `search_artifacts()` BEFORE any web_search, download, or analysis!
    
    - **search_artifactsëŠ” ê¸°ë³¸ì ìœ¼ë¡œ í˜„ì¬ ì„¸ì…˜ì˜ Artifactë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤**
    - If search_artifacts returns relevant data â†’ USE IT, DO NOT re-download!
    - Only search externally if search_artifacts returns no matching results
    - Results from web_search, read_downloaded_file, summarize_document are AUTO-SAVED
    - ë‹¤ë¥¸ ì„¸ì…˜ì˜ ë°ì´í„°ê°€ í•„ìš”í•˜ë©´ current_session_only=False ì‚¬ìš©
    
    **VIOLATION**: Downloading data that already exists in artifacts is FORBIDDEN!

11. **Text-to-SQL (PostgreSQL meetingroom ë°ì´í„°ë² ì´ìŠ¤)**:
    - Use text_to_sql for natural language queries about meeting rooms, reservations, etc.
    - Schema is stored in Neo4j (ObjectType + Column nodes with vector embeddings)
    - Use search_table_schema first if you need to understand the schema
    - Use get_postgres_tables to see all available tables
    - Use run_postgres_sql for direct SQL execution when you know the exact query

12. Always show the LaTeX formula in your response.
13. Respond in Korean when the user asks in Korean.
14. Explain the actuarial reasoning behind your answers.

## ğŸš¨ MANDATORY Workflow for ALL Data/Research Requests:

### Step 1: ALWAYS CHECK EXISTING DATA FIRST (í•„ìˆ˜!)
```
search_artifacts("ê´€ë ¨ í‚¤ì›Œë“œ")  â† ì´ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ë©´ ì•ˆë¨!
```

### Step 2: Based on search_artifacts result (í˜„ì¬ ì„¸ì…˜ ë‚´ ê²€ìƒ‰):
**IF found relevant artifacts in current session:**
- Use get_artifact_content(artifact_id) to retrieve full content
- Analyze/summarize from existing data
- DO NOT download again!

**IF NO relevant artifacts found:**
```
1. web_search("ê²€ìƒ‰ì–´") - ì›¹ ê²€ìƒ‰ (ìë™ ì €ì¥ë¨)
2. download_file(url) - PDF/Excel ë‹¤ìš´ë¡œë“œ
3. read_downloaded_file(filepath) - íŒŒì¼ íŒŒì‹± (ìë™ ì €ì¥ë¨)
4. summarize_document(filepath, "ì£¼ì œ") - AI ìš”ì•½ (ìë™ ì €ì¥ë¨)
5. ì‚¬ìš©ìì—ê²Œ í•µì‹¬ ìš”ì•½ ë° ë°ì´í„° ì œì‹œ
```

### âŒ WRONG (ì ˆëŒ€ ê¸ˆì§€):
- search_artifacts ì—†ì´ ë°”ë¡œ web_search í˜¸ì¶œ
- ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì„ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ
- ì´ë¯¸ ë¶„ì„ëœ ë‚´ìš©ì„ ë‹¤ì‹œ ë¶„ì„

### âœ… CORRECT (ì˜¬ë°”ë¥¸ í–‰ë™):
- ë¨¼ì € search_artifactsë¡œ ê¸°ì¡´ ë°ì´í„° í™•ì¸
- ìˆìœ¼ë©´ ì¬ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒˆë¡œ ê²€ìƒ‰/ë‹¤ìš´ë¡œë“œ
"""

def run_agent(user_query: str):
    initial_state = {"messages": [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=user_query)
    ]}
    result = graph.invoke(initial_state)
    return result["messages"][-1].content


import asyncio
from queue import Queue
from threading import Thread
from langchain_core.callbacks import BaseCallbackHandler

class StreamingCallbackHandler(BaseCallbackHandler):
    """Callback handler for streaming LLM tokens."""
    
    def __init__(self, queue: Queue):
        self.queue = queue
    
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """Called when LLM generates a new token."""
        if token:
            self.queue.put({"type": "token", "content": token})
    
    def on_tool_start(self, serialized, input_str, **kwargs) -> None:
        """Called when a tool starts running."""
        tool_name = serialized.get("name", "unknown")
        self.queue.put({"type": "tool_start", "tool": tool_name, "input": str(input_str)[:200]})
    
    def on_tool_end(self, output, **kwargs) -> None:
        """Called when a tool finishes running."""
        self.queue.put({"type": "tool_end", "output": str(output)[:500] if output else ""})


def _run_agent_sync(user_query: str, queue: Queue):
    """Synchronous agent runner that puts results in a queue."""
    try:
        callback_handler = StreamingCallbackHandler(queue)
        
        # Create LLM with streaming callback
        streaming_llm = ChatOpenAI(
            model="gpt-4.1", 
            api_key=OPENAI_API_KEY, 
            temperature=0, 
            streaming=True,
            callbacks=[callback_handler]
        )
        streaming_llm_with_tools = streaming_llm.bind_tools(tools)
        
        def streaming_agent_node(state: AgentState):
            return {"messages": [streaming_llm_with_tools.invoke(state["messages"])]}
        
        # Build a new graph with streaming
        streaming_workflow = StateGraph(AgentState)
        streaming_workflow.add_node("agent", streaming_agent_node)
        streaming_workflow.add_node("tools", ToolNode(tools))
        streaming_workflow.set_entry_point("agent")
        streaming_workflow.add_conditional_edges("agent", tools_condition)
        streaming_workflow.add_edge("tools", "agent")
        streaming_graph = streaming_workflow.compile()
        
        initial_state = {"messages": [
            SystemMessage(content=SYSTEM_PROMPT),
            HumanMessage(content=user_query)
        ]}
        
        # Run the graph
        streaming_graph.invoke(initial_state, config={"callbacks": [callback_handler]})
        
    except Exception as e:
        queue.put({"type": "error", "content": str(e)})
    finally:
        queue.put({"type": "done"})


async def run_agent_stream(user_query: str):
    """
    Streaming version of run_agent.
    Yields chunks of the LLM response as they are generated.
    """
    queue = Queue()
    
    # Run the synchronous agent in a separate thread
    thread = Thread(target=_run_agent_sync, args=(user_query, queue))
    thread.start()
    
    # Yield items from the queue
    while True:
        # Check queue with a small timeout to allow async context switches
        await asyncio.sleep(0.01)
        
        while not queue.empty():
            item = queue.get()
            yield item
            
            if item.get("type") == "done" or item.get("type") == "error":
                thread.join(timeout=1)
                return
        
        # Check if thread is still alive
        if not thread.is_alive() and queue.empty():
            yield {"type": "done"}
            return
